#!/bin/bash
# Unified sync script - Pull all external data sources
# This script synchronizes all data sources to prepare for AI review

set -e

# Load common dotenvx handler
source "$(dirname "$0")/lib/dotenvx-loader.sh"
auto_dotenvx "$@"

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}âœ“${NC} $1"
}

print_error() {
    echo -e "${RED}âœ—${NC} $1"
}

print_info() {
    echo -e "${BLUE}â„¹${NC} $1"
}

print_header() {
    echo ""
    echo -e "${YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${YELLOW}$1${NC}"
    echo -e "${YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
}

# Track what was synced
SYNC_SUMMARY=""

# Helper function to run a sync step with timing
run_sync_step() {
    local header="$1"
    local func="$2"
    local success_msg="$3"
    local summary_name="$4"
    
    print_header "$header"
    START_TIME=$(date +%s)
    if $func; then
        print_status "$success_msg"
        SYNC_SUMMARY="${SYNC_SUMMARY}âœ“ $summary_name\n"
    else
        print_error "$summary_name sync failed"
        SYNC_SUMMARY="${SYNC_SUMMARY}âœ— $summary_name\n"
    fi
    END_TIME=$(date +%s)
    print_info "â±ï¸  Took $((END_TIME - START_TIME)) seconds"
    # Always return 0 to prevent script exit with set -e
    return 0
}

# Main sync function
main() {
    echo ""
    echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${BLUE}â•‘       TODAY - Full Data Sync          â•‘${NC}"
    echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
    echo "Synchronizing all data sources..."
    echo "Started at: $(date '+%Y-%m-%d %H:%M:%S')"
    
    # Run all operations synchronously for reliability
    print_info "Running all sync operations synchronously..."
    
    # Critical path operations (fast, important)
    run_sync_step "â˜ï¸ Turso Pull" sync_turso "Turso pulled" "Turso pull"
    run_sync_step "ðŸ“ GitHub Vault" sync_github_vault "GitHub vault synced" "GitHub vault"
    run_sync_step "ðŸ§¹ Clean Markdown" clean_markdown_files "Markdown cleaned" "Markdown cleanup"
    
    # Task sync without classification
    SKIP_CLASSIFICATION=1 run_sync_step "âœ… Task Manager" sync_tasks "Tasks synced" "Task manager"
    run_sync_step "ðŸ“¥ Process Inbox" process_inbox "Inbox processed" "Inbox processed"
    run_sync_step "âœ… Archive Tasks" archive_completed_tasks "Tasks archived" "Tasks archived"
    
    # Run additional operations synchronously (no background jobs)
    print_header "âš¡ Additional Sync Operations"
    
    # Task classification in background (it's slow and non-critical)
    print_info "ðŸ” Starting task classification in background..."
    {
        bin/tasks classify-stages > /tmp/classify-$$.log 2>&1
        if [ $? -eq 0 ]; then
            echo "âœ… Task classification completed successfully" >> /tmp/classify-$$.log
        else
            echo "âŒ Task classification failed" >> /tmp/classify-$$.log
        fi
    } &
    CLASSIFY_PID=$!
    print_info "   Classification running in background (PID: $CLASSIFY_PID)"
    print_info "   Check progress: tail -f /tmp/classify-$$.log"
    
    # Email sync (if configured)
    if grep -q "EMAIL_ACCOUNT=" .env 2>/dev/null && grep -q "EMAIL_PASSWORD=" .env 2>/dev/null; then
        run_sync_step "ðŸ“§ Email Sync" sync_email "Email synced" "Email sync"
    fi
    
    # Calendar sync
    run_sync_step "ðŸ“… Calendar Sync" sync_calendar "Calendar synced" "Calendar sync"
    
    # Contacts sync
    run_sync_step "ðŸ‘¥ Contacts Sync" sync_contacts "Contacts synced" "Contacts sync"
    
    # Push to Turso
    run_sync_step "â˜ï¸ Turso Push" push_to_turso "Turso pushed" "Turso push"
    
    print_status "All sync operations completed (classification running in background)."
    
    # Print summary
    print_header "ðŸ“‹ Sync Summary"
    echo -e "$SYNC_SUMMARY"
    echo "Completed at: $(date '+%Y-%m-%d %H:%M:%S')"
    echo ""
    
    # Check if any sync failed
    if echo -e "$SYNC_SUMMARY" | grep -q "âœ—"; then
        print_error "Some data sources failed to sync"
        echo "Please configure missing services in .env file"
        exit 1
    else
        print_status "All data sources synchronized!"
    fi
    
    echo ""
    if [ ! -z "$CLASSIFY_PID" ]; then
        echo "Note: Task classification is still running in background (PID: $CLASSIFY_PID)"
        echo "      Check progress: tail -f /tmp/classify-$$.log"
    fi
    echo ""
    echo "Next step: Run 'bin/today' to get AI suggestions on what to do today"
}

# Sync GitHub vault
sync_github_vault() {
    # Change to project directory
    cd "$(dirname "$0")/.." || return 1
    
    # Update file dates index first
    if [[ -x bin/update-file-dates-index ]]; then
        bin/update-file-dates-index > /dev/null 2>&1
    fi
    
    # First, check if we can pull cleanly (no uncommitted changes in repo)
    if [[ -n $(git status --porcelain 2>/dev/null) ]]; then
        # Check if there are changes ONLY in vault/ (no other staged or unstaged changes)
        CONTENT_CHANGES=$(git status vault/ --porcelain 2>/dev/null)
        OTHER_CHANGES=$(git status --porcelain 2>/dev/null | grep -v "^.. vault/" || true)
        
        if [[ -n "$CONTENT_CHANGES" ]] && [[ -z "$OTHER_CHANGES" ]]; then
            # Only content changes, safe to commit
            print_info "Pushing local content changes..."
            git add vault/ 2>/dev/null
            git commit -m "Sync vault content" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
        elif [[ -n "$CONTENT_CHANGES" ]]; then
            # We have both content changes and other changes
            # First, stash any staged changes outside of vault/
            if [[ -n $(git diff --cached --name-only | grep -v "^vault/" || true) ]]; then
                print_info "Saving staged changes outside vault..."
                # Create a temporary commit to preserve staged state
                git commit --quiet -m "temp: staged changes" -- ':!vault/' 2>/dev/null || true
                TEMP_COMMIT=true
            fi
            
            # Now commit just the content
            print_info "Pushing local content changes..."
            git add vault/ 2>/dev/null
            git commit -m "Sync vault content" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
            
            # Restore the temporary commit if we made one
            if [[ "${TEMP_COMMIT:-false}" == "true" ]]; then
                print_info "Restoring staged changes..."
                git reset --soft HEAD~1 2>/dev/null
            fi
        else
            # Changes in other files but not content - stash them temporarily
            print_info "Stashing local changes outside content directories..."
            git stash push --quiet -m "sync-auto-stash" -- ':!vault/' 2>/dev/null
            STASHED=true
        fi
    fi
    
    # Pull latest changes from remote
    print_info "Pulling latest changes from GitHub..."
    if git pull origin main --quiet 2>/dev/null; then
        PULL_RESULT=$(git log --oneline -1 --format="%s")
        if [[ "$PULL_RESULT" != *"Already up to date"* ]]; then
            print_status "Pulled latest changes from GitHub"
        fi
    else
        # Pull failed - check if it's a merge conflict
        if git status | grep -q "You have unmerged paths"; then
            print_error "Merge conflict detected - manual resolution needed"
            print_info "Run 'git status' to see conflicts"
            return 1
        else
            print_info "No remote changes to pull"
        fi
    fi
    
    # Restore stashed changes if we stashed them
    if [[ "${STASHED:-false}" == "true" ]]; then
        print_info "Restoring stashed changes..."
        git stash pop --quiet 2>/dev/null || true
    fi
    
    # Clean up empty files in all content directories
    find vault -type f -empty -delete 2>/dev/null
    deleted_count=$(git status --porcelain vault/ | grep -c "^ D " || true)
    if [[ $deleted_count -gt 0 ]]; then
        print_info "Removed $deleted_count empty file(s)"
        git add vault/
        git commit -m "Clean up empty vault files" --quiet 2>/dev/null
        git push origin main --quiet 2>/dev/null
    fi
    
    # Show content counts
    note_count=$(find vault/notes -type f -name "*.md" 2>/dev/null | wc -l)
    plan_count=$(find vault/plans -type f -name "*.md" 2>/dev/null | wc -l)
    project_count=$(find vault/projects -type f -name "*.md" 2>/dev/null | wc -l)
    topic_count=$(find vault/topics -type f -name "*.md" 2>/dev/null | wc -l)
    print_info "Total notes: $note_count, plans: $plan_count, projects: $project_count, topics: $topic_count"
    
    # Count OGM work files
    ogm_count=$(find vault/notes/ogm-work -type f -name "*.md" 2>/dev/null | wc -l || echo 0)
    if [[ $ogm_count -gt 0 ]]; then
        print_info "OlderGay.Men work summaries: $ogm_count"
    fi
    
    # Show recent content (last 7 days) across all directories
    recent_notes=$(find vault/notes -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_plans=$(find vault/plans -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_projects=$(find vault/projects -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_topics=$(find vault/topics -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_count=$((recent_notes + recent_plans + recent_projects + recent_topics))
    
    if [[ $recent_count -gt 0 ]]; then
        print_info "Content from last 7 days: $recent_count files (notes: $recent_notes, plans: $recent_plans, projects: $recent_projects, topics: $recent_topics)"
        # Show up to 5 most recent files across all directories
        print_info "Most recent content:"
        find vault -type f -name "*.md" -mtime -7 2>/dev/null | \
            xargs ls -t 2>/dev/null | head -5 | while read -r file; do
            echo "  â€¢ $(basename "$file") ($(dirname "$file"))"
        done
    fi
    
    return 0
}

# Clean markdown files with markdownlint
clean_markdown_files() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Cleaning markdown files with markdownlint..."
    
    # Count markdown files to process
    MD_COUNT=$(find vault -type f -name "*.md" 2>/dev/null | wc -l)
    
    if [[ $MD_COUNT -eq 0 ]]; then
        print_info "No markdown files found to clean"
        return 0
    fi
    
    print_info "Processing $MD_COUNT markdown files..."
    
    # Run markdownlint-cli2 with fix option
    # Using a glob pattern to process all markdown files in our content directories
    output=$(npx markdownlint-cli2 --fix "vault/**/*.md" 2>&1) || true
    
    # Check if any files were fixed
    if echo "$output" | grep -q "Fixing:"; then
        fixed_count=$(echo "$output" | grep -c "Fixing:" || echo 0)
        print_info "Fixed formatting in $fixed_count file(s)"
        
        # Commit the cleaned files
        if [[ -n $(git status --porcelain vault/ 2>/dev/null) ]]; then
            print_info "Committing markdown formatting fixes..."
            git add vault/ 2>/dev/null
            git commit -m "Clean up markdown formatting with markdownlint" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
        fi
    else
        print_info "All markdown files are already clean"
    fi
    
    return 0
}

# Check task consistency across all markdown files
check_task_consistency() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Checking task consistency across markdown files..."
    
    # Run the consistency check with auto-fix
    output=$(bin/tasks check-consistency --fix 2>&1)
    
    if echo "$output" | grep -q "All tasks are consistent"; then
        print_info "All tasks are consistent"
        return 0
    elif echo "$output" | grep -q "Fixed.*inconsistent task"; then
        # Extract the number of fixed tasks
        fixed_count=$(echo "$output" | grep -oE "Fixed [0-9]+ inconsistent" | grep -oE "[0-9]+")
        print_info "Fixed $fixed_count inconsistent task(s)"
        return 0
    else
        print_error "Task consistency check failed"
        return 1
    fi
}

# Sync task manager (includes stage classification)
sync_tasks() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Syncing task database with markdown files..."
    
    # Run the task sync command (now includes stage classification)
    output=$(bin/tasks sync 2>&1)
    
    if [[ $? -eq 0 ]]; then
        # Show the sync output but filter for key info
        echo "$output" | while IFS= read -r line; do
            # Highlight stage classification results
            if echo "$line" | grep -qE "Classified|Stage|Front|Back|Off"; then
                print_info "$line"
            elif echo "$line" | grep -q "âœ“"; then
                print_info "$line"
            elif echo "$line" | grep -q "Generated today.md"; then
                print_info "$line"
            fi
        done
        return 0
    else
        print_error "Task sync failed"
        echo "$output" | while IFS= read -r line; do
            print_info "$line"
        done
        return 1
    fi
}

# Sync with Turso cloud database
sync_turso() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if Turso is configured
    if [[ -z "$TURSO_DATABASE_URL" ]] || [[ -z "$TURSO_AUTH_TOKEN" ]]; then
        print_info "Turso not configured (need TURSO_DATABASE_URL and TURSO_AUTH_TOKEN)"
        return 1
    fi
    
    # Use the simplified turso-sync script
    print_info "Syncing with Turso cloud database..."
    
    # Run the turso-sync script with pull action and timeout
    if timeout 30 bin/turso-sync pull 2>&1 | while IFS= read -r line; do
        if [[ "$line" == *"âœ…"* ]] || [[ "$line" == *"â¬‡ï¸"* ]] || [[ "$line" == *"Changes pulled"* ]]; then
            print_info "$line"
        elif [[ "$line" == *"âœ—"* ]] || [[ "$line" == *"Error"* ]] || [[ "$line" == *"Failed"* ]]; then
            print_error "$line"
        fi
    done; then
        return 0
    else
        local exit_code=$?
        if [[ $exit_code -eq 124 ]]; then
            print_info "Turso pull timed out after 30s - continuing with local data"
            return 0  # Don't fail the sync, just continue
        else
            print_error "Turso pull failed"
            return 1
        fi
    fi
}

# Push changes to Turso cloud database (no pull)
push_to_turso() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if Turso is configured
    if [[ -z "$TURSO_DATABASE_URL" ]] || [[ -z "$TURSO_AUTH_TOKEN" ]]; then
        print_info "Turso not configured (need TURSO_DATABASE_URL and TURSO_AUTH_TOKEN)"
        return 1
    fi
    
    # Use the simplified turso-sync script
    print_info "Pushing changes to Turso..."
    
    # Run the turso-sync script with push action
    if bin/turso-sync push 2>&1 | while IFS= read -r line; do
        if [[ "$line" == *"âœ…"* ]] || [[ "$line" == *"â¬†ï¸"* ]] || [[ "$line" == *"Changes pushed"* ]]; then
            print_info "$line"
        elif [[ "$line" == *"âœ—"* ]] || [[ "$line" == *"Error"* ]] || [[ "$line" == *"Failed"* ]]; then
            print_error "$line"
        fi
    done; then
        return 0
    else
        return 1
    fi
}

# Sync Notion database
sync_notion() {
    # Use the notion CLI to refresh cache
    cd "$(dirname "$0")/.." || return 1
    
    # Check if NOTION_TOKEN is configured
    if ! grep -q "NOTION_TOKEN=" .env 2>/dev/null || grep -q "NOTION_TOKEN=your_notion" .env 2>/dev/null; then
        print_error "Notion not configured"
        print_info "Add NOTION_TOKEN to .env file"
        return 1
    fi
    
    # Set up cache database
    CACHE_DB=".data/today.db"
    
    # Ensure sync_metadata table exists
    if [[ -f "$CACHE_DB" ]]; then
        sqlite3 "$CACHE_DB" "CREATE TABLE IF NOT EXISTS sync_metadata (key TEXT PRIMARY KEY, value TEXT)" 2>/dev/null
        LAST_SYNC=$(sqlite3 "$CACHE_DB" "SELECT value FROM sync_metadata WHERE key='last_full_sync'" 2>/dev/null || echo "")
    else
        print_info "No cache found, initial sync needed..."
        LAST_SYNC=""
    fi
    
    # For now, use the regular fetch-tasks which is fast when cached
    print_info "Syncing Notion tasks..."
    
    # Fetch tasks (will use cache intelligently)
    task_output=$(bin/notion fetch-tasks 2>&1 | tail -5)
    if [[ $? -eq 0 ]]; then
        echo "$task_output" | while IFS= read -r line; do
            print_info "$line"
        done
        
        # Update sync timestamp
        sqlite3 "$CACHE_DB" "INSERT OR REPLACE INTO sync_metadata (key, value) VALUES ('last_full_sync', datetime('now'))" 2>/dev/null
        
        return 0
    else
        print_error "Notion connection failed"
        if echo "$output" | grep -q "NOTION_TOKEN"; then
            print_info "Please configure NOTION_TOKEN in .env file"
        else
            print_info "Error: $output"
        fi
        return 1
    fi
}

# Sync email
sync_email() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if email-cli/email binary exists
    EMAIL_BIN=""
    if [[ -f bin/email ]]; then
        EMAIL_BIN="bin/email"
    elif [[ -f bin/email-cli ]]; then
        EMAIL_BIN="bin/email-cli"
    else
        print_error "Email CLI not available"
        return 1
    fi
    
    # Check if email credentials are configured
    if ! grep -q "EMAIL_ACCOUNT=" .env 2>/dev/null || ! grep -q "EMAIL_PASSWORD=" .env 2>/dev/null; then
        print_error "Email not configured"
        print_info "Run 'email setup' for configuration instructions"
        return 1
    fi
    
    # Incremental email sync - fetch emails newer than what we have
    if [[ -f .data/today.db ]]; then
        # Get the timestamp of the most recent email
        EMAIL_INFO=$(sqlite3 .data/today.db "SELECT MAX(date), COUNT(*) FROM emails" 2>/dev/null || echo "||0")
        LAST_EMAIL_DATE=$(echo "$EMAIL_INFO" | cut -d'|' -f1)
        EMAIL_COUNT=$(echo "$EMAIL_INFO" | cut -d'|' -f2)
        
        if [[ -n "$LAST_EMAIL_DATE" ]] && [[ "$EMAIL_COUNT" -gt 0 ]]; then
            # Calculate days since last email to determine sync range
            DAYS_OLD=$(node -e "
                const date = new Date('$LAST_EMAIL_DATE');
                const now = new Date();
                const daysSince = (now - date) / (1000 * 60 * 60 * 24);
                console.log(Math.ceil(daysSince));
            " 2>/dev/null || echo "7")
            
            # Sync enough days to catch all new emails since last sync
            # Add 1 day buffer to ensure we don't miss anything
            DAYS_TO_SYNC=$((DAYS_OLD + 1))
            
            # Cap at 7 days max for performance
            if [[ $DAYS_TO_SYNC -gt 7 ]]; then
                DAYS_TO_SYNC=7
                print_info "Syncing emails from last 7 days (max range)..."
            else
                print_info "Syncing emails from last $DAYS_TO_SYNC days to catch all new mail..."
            fi
        else
            print_info "No emails in database, initial sync of 7 days..."
            DAYS_TO_SYNC=7
        fi
    else
        print_info "No email database found, initial sync of 7 days..."
        DAYS_TO_SYNC=7
    fi
    
    # Use timeout based on days with 30 second base
    TIMEOUT=$((30 + $DAYS_TO_SYNC * 5))  # 30s base + 5s per day
    print_info "Fetching new emails (timeout: ${TIMEOUT}s)..."
    
    # Create a temp file for output
    EMAIL_OUTPUT=$(mktemp)
    
    # Run email download in background with timeout, showing progress
    timeout $TIMEOUT $EMAIL_BIN download --days $DAYS_TO_SYNC > "$EMAIL_OUTPUT" 2>&1 &
    EMAIL_PID=$!
    
    # Monitor the output file for "already in progress" or transaction error messages
    WAIT_COUNT=0
    while kill -0 $EMAIL_PID 2>/dev/null; do
        if grep -q "Another download is already in progress" "$EMAIL_OUTPUT" 2>/dev/null; then
            print_info "Another email download is already in progress, skipping..."
            kill $EMAIL_PID 2>/dev/null || true
            wait $EMAIL_PID 2>/dev/null || true
            rm -f "$EMAIL_OUTPUT"
            return 0
        fi
        
        # Check for database transaction errors
        if grep -q "this.cache.db.transaction is not a function" "$EMAIL_OUTPUT" 2>/dev/null; then
            print_error "Email database error detected"
            print_info "Try running 'bin/email repair' to fix the database"
            kill $EMAIL_PID 2>/dev/null || true
            wait $EMAIL_PID 2>/dev/null || true
            rm -f "$EMAIL_OUTPUT"
            return 1
        fi
        
        # Show any new output lines (for progress)
        if [[ -f "$EMAIL_OUTPUT" ]] && [[ -s "$EMAIL_OUTPUT" ]]; then
            tail -n +$((WAIT_COUNT + 1)) "$EMAIL_OUTPUT" 2>/dev/null | while IFS= read -r line; do
                if [[ -n "$line" ]]; then
                    echo "  $line"
                fi
            done
            WAIT_COUNT=$(wc -l < "$EMAIL_OUTPUT" 2>/dev/null || echo 0)
        fi
        
        sleep 0.5
    done
    
    # Get exit code
    wait $EMAIL_PID
    email_exit_code=$?
    
    # Read final output
    output=$(cat "$EMAIL_OUTPUT" 2>/dev/null || echo "")
    rm -f "$EMAIL_OUTPUT"
    
    # Check for timeout
    if [[ $email_exit_code -eq 124 ]]; then
        print_error "Email download timed out after $TIMEOUT seconds"
        print_info "Try running 'email download --days 1' for a quicker sync"
        return 1
    elif [[ $email_exit_code -ne 0 ]]; then
        print_error "Email download failed"
        return 1
    fi
    
    # Show download results
    if echo "$output" | grep -q "Downloaded"; then
        # Extract number of emails downloaded
        email_count=$(echo "$output" | grep -oE "Downloaded [0-9]+ email" | grep -oE "[0-9]+") || true
        if [[ -n "$email_count" ]]; then
            print_info "Downloaded $email_count new emails"
        fi
    else
        print_info "No new emails to download"
    fi
    
    # Show email stats
    stats_output=$($EMAIL_BIN stats 2>&1 | grep "Total emails:" | cut -d: -f2 | xargs) || true
    if [[ -n "$stats_output" ]]; then
        print_info "Total emails in database: $stats_output"
    fi
    
    return 0
}

# Sync calendar events
sync_calendar() {
    cd "$(dirname "$0")/.." || return 1
    
    # Run calendar sync
    print_info "Syncing calendar events..."
    SYNC_OUTPUT=$(bin/calendar sync 2>&1)
    
    # Check if sync was successful
    if echo "$SYNC_OUTPUT" | grep -q "âœ… Synced [0-9]"; then
        # Extract event count
        EVENT_COUNT=$(echo "$SYNC_OUTPUT" | grep -oE "âœ… Synced [0-9]+ calendar" | grep -oE "[0-9]+" || echo 0)
        if [[ "$EVENT_COUNT" -gt 0 ]]; then
            print_info "Synced $EVENT_COUNT calendar events"
            # Show quick summary of today's events
            TODAY_COUNT=$(bin/calendar today 2>/dev/null | jq '. | length' 2>/dev/null || echo 0)
            if [[ "$TODAY_COUNT" -gt 0 ]]; then
                print_info "$TODAY_COUNT events scheduled for today"
            fi
            return 0
        else
            print_info "No calendar events found"
            return 0
        fi
    else
        print_info "No calendars configured (run 'bin/setup --calendar' for instructions)"
        return 1
    fi
}

# Sync contacts
sync_contacts() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if contacts binary exists
    if [[ ! -f bin/contacts ]]; then
        print_error "Contacts CLI not available"
        return 1
    fi
    
    # Check if iCloud credentials are configured
    if ! grep -q "ICLOUD_USERNAME=" .env 2>/dev/null || ! grep -q "ICLOUD_APP_PASSWORD=" .env 2>/dev/null; then
        print_info "iCloud contacts not configured (need ICLOUD_USERNAME and ICLOUD_APP_PASSWORD)"
        return 1
    fi
    
    # Run contacts sync
    print_info "Syncing iCloud contacts..."
    
    sync_output=$(bin/contacts sync 2>&1)
    if [[ $? -eq 0 ]]; then
        # Extract contact count from output
        # Check for synced to database message
        if echo "$sync_output" | grep -q "âœ… Synced [0-9]* contacts to database"; then
            contact_count=$(echo "$sync_output" | grep -oE "[0-9]+ contacts to database" | grep -oE "[0-9]+" | head -1)
            print_info "Synced $contact_count contacts to database"
        # Check for database cache message (new SQLite version)
        elif echo "$sync_output" | grep -q "Loaded [0-9]* contacts from database cache"; then
            contact_count=$(echo "$sync_output" | grep -oE "[0-9]+ contacts from database cache" | grep -oE "[0-9]+" | head -1)
            print_info "Loaded $contact_count contacts from cache"
        # Fallback for old cache message
        elif echo "$sync_output" | grep -q "Loaded [0-9]* contacts from cache"; then
            contact_count=$(echo "$sync_output" | grep -oE "[0-9]+ contacts from cache" | grep -oE "[0-9]+" | head -1)
            print_info "Loaded $contact_count contacts from cache"
        else
            print_info "No contacts found"
        fi
        
        # Show stats
        stats_output=$(bin/contacts stats 2>&1 | grep -E "Total|With email" | head -2)
        if [[ -n "$stats_output" ]]; then
            echo "$stats_output" | while IFS= read -r line; do
                print_info "  $line"
            done
        fi
        
        return 0
    else
        print_error "Contacts sync failed"
        if echo "$sync_output" | grep -q "Authentication failed"; then
            print_info "Check your iCloud app-specific password"
        else
            print_info "Error: $sync_output"
        fi
        return 1
    fi
}

# Process inbox notes (from Drafts uploads)
process_inbox() {
    cd "$(dirname "$0")/.." || return 1
    
    # Create inbox directory if it doesn't exist
    mkdir -p vault/notes/inbox 2>/dev/null
    
    # Check if there are any files in the inbox
    INBOX_FILES=$(find vault/notes/inbox -type f -name "*.md" 2>/dev/null)
    if [[ -z "$INBOX_FILES" ]]; then
        print_info "No files in inbox to process"
        return 0
    fi
    
    # Count files to process
    FILE_COUNT=$(echo "$INBOX_FILES" | wc -l)
    print_info "Processing $FILE_COUNT file(s) from inbox..."
    
    # Process each file
    while IFS= read -r file; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        BASENAME=$(basename "$file")
        CONTENT=$(cat "$file" 2>/dev/null || echo "")
        FIRST_LINE=$(echo "$CONTENT" | head -1)
        TITLE=$(echo "$FIRST_LINE" | sed 's/^#\s*//' | sed 's/^-\s*\[\s*\]\s*//')
        
        # Determine destination based on content
        DEST_DIR=""
        DEST_FILE=""
        
        # Special case: Streaks file
        if [[ "$TITLE" == "Streaks" ]]; then
            # Convert Streaks items to TaskPaper format if needed
            CONVERTED_CONTENT=$(echo "$CONTENT" | awk '
                NR==1 {print; next}  # Keep header
                /^[^-]/ && /[a-zA-Z]/ {print "- [ ] " $0; next}  # Add checkbox to non-checkbox lines
                {print}  # Keep other lines as-is
            ')
            DEST_DIR="vault/notes/tasks"
            DEST_FILE="streaks-today.md"
            echo "$CONVERTED_CONTENT" > "$file"  # Update content for move
            print_info "  â€¢ Streaks file â†’ tasks/streaks-today.md"
        
        # Special case: Progress note - append to today's review
        elif [[ "$TITLE" == "Progress" ]]; then
            # Get date components for new naming scheme
            YEAR=$(date +%Y)
            MONTH=$(date +%m)
            DAY=$(date +%d)
            # Determine quarter from month
            if [ $MONTH -le 3 ]; then Q="Q1"
            elif [ $MONTH -le 6 ]; then Q="Q2"
            elif [ $MONTH -le 9 ]; then Q="Q3"
            else Q="Q4"
            fi
            REVIEW_FILE="vault/plans/${YEAR}-${Q}-${MONTH}-${DAY}.md"
            TIMESTAMP=$(date +"%H:%M")
            
            # Extract content (everything after first line)
            PROGRESS_CONTENT=$(echo "$CONTENT" | tail -n +2 | sed '/^$/d')
            
            if [[ -f "$REVIEW_FILE" ]]; then
                # Append to review file
                echo "" >> "$REVIEW_FILE"
                echo "### Progress Update ($TIMESTAMP)" >> "$REVIEW_FILE"
                echo "$PROGRESS_CONTENT" >> "$REVIEW_FILE"
                print_info "  â€¢ Progress note â†’ Added to today's review"
                print_info "    $TIMESTAMP: $(echo "$PROGRESS_CONTENT" | head -1)"
            else
                print_error "  â€¢ No review file for today. Run 'bin/today' first"
            fi
            
            # Delete the progress note from inbox
            rm "$file"
            continue  # Skip the normal file move logic
        
        # Special case: Concerns file
        elif [[ "$TITLE" == "Concerns" ]] || [[ "$BASENAME" == *"concerns"* ]]; then
            # Remove title line and subsequent blank lines
            CLEANED_CONTENT=$(echo "$CONTENT" | awk '
                NR==1 && /^#.*[Cc]oncerns/ {next}  # Skip title line if it contains "concerns"
                /^$/ && !started {next}            # Skip leading blank lines
                {started=1; print}                 # Print everything else
            ')
            echo "$CLEANED_CONTENT" > "$file"  # Update content for move
            DEST_DIR="vault/notes/concerns"
            DEST_FILE="$BASENAME"
            print_info "  â€¢ Concerns file â†’ concerns/$BASENAME"
        
        # Task files (contains checkboxes)
        elif echo "$CONTENT" | grep -qE "^-\s*\[[ x]\]"; then
            # Check if this file contains ONLY tasks (no other content)
            TASK_ONLY=true
            while IFS= read -r line; do
                # Skip empty lines
                if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*$ ]]; then
                    continue
                fi
                # Check if line is a task
                if ! echo "$line" | grep -qE "^-\s*\[[ x]\]"; then
                    TASK_ONLY=false
                    break
                fi
            done <<< "$CONTENT"
            
            if [[ "$TASK_ONLY" == "true" ]]; then
                # Append to consolidated tasks.md file
                TASKS_FILE="vault/notes/tasks/tasks.md"
                mkdir -p vault/notes/tasks 2>/dev/null
                
                # Insert tasks before Archive section (or at end if no Archive)
                if [[ -f "$TASKS_FILE" ]]; then
                    if grep -q "^# Archive" "$TASKS_FILE"; then
                        # File has Archive section - insert before it
                        TEMP_FILE=$(mktemp)
                        
                        # Get everything before Archive
                        awk '/^# Archive/ {exit} {print}' "$TASKS_FILE" > "$TEMP_FILE"
                        
                        # Remove trailing blank lines from active section
                        sed -i '' -e :a -e '/^\s*$/d;N;ba' "$TEMP_FILE" 2>/dev/null || \
                        sed -i -e :a -e '/^\s*$/d;N;ba' "$TEMP_FILE" 2>/dev/null || true
                        
                        # Add the new tasks
                        echo "" >> "$TEMP_FILE"
                        echo "$CONTENT" >> "$TEMP_FILE"
                        
                        # Add Archive section back
                        echo "" >> "$TEMP_FILE"
                        awk '/^# Archive/,EOF {print}' "$TASKS_FILE" >> "$TEMP_FILE"
                        
                        # Replace original file
                        mv "$TEMP_FILE" "$TASKS_FILE"
                    else
                        # No Archive section - just append at end
                        if [[ -n "$(tail -c 1 "$TASKS_FILE")" ]]; then
                            echo "" >> "$TASKS_FILE"
                        fi
                        echo "$CONTENT" >> "$TASKS_FILE"
                    fi
                else
                    # File doesn't exist - create it
                    echo "$CONTENT" > "$TASKS_FILE"
                fi
                
                print_info "  â€¢ Task-only file â†’ Added to tasks/tasks.md"
                print_info "    Added $(echo "$CONTENT" | grep -c "^-\s*\[[ x]\]") task(s)"
                
                # Delete the original file from inbox
                rm "$file"
                continue  # Skip the normal file move logic
            else
                # Mixed content - keep as separate file
                DEST_DIR="vault/notes/tasks"
                DEST_FILE="$BASENAME"
                print_info "  â€¢ Mixed task file â†’ tasks/$BASENAME"
            fi
        
        # Daily notes (default)
        else
            DEST_DIR="vault/notes/daily"
            DEST_FILE="$BASENAME"
            print_info "  â€¢ Daily note â†’ daily/$BASENAME"
        fi
        
        # Create destination directory if needed
        mkdir -p "$DEST_DIR" 2>/dev/null
        
        # Move the file
        DEST_PATH="$DEST_DIR/$DEST_FILE"
        if [[ -f "$DEST_PATH" ]] && [[ "$DEST_FILE" != "streaks-today.md" ]]; then
            # File exists and it's not streaks (which we always overwrite)
            print_info "    File exists, skipping: $DEST_FILE"
            rm "$file"  # Remove from inbox since it's a duplicate
        else
            mv "$file" "$DEST_PATH" 2>/dev/null && {
                print_info "    âœ“ Filed to $DEST_PATH"
            } || {
                print_error "    Failed to move $BASENAME"
            }
        fi
    done <<< "$INBOX_FILES"
    
    # Commit any moved files
    if [[ -n $(git status --porcelain vault/ 2>/dev/null) ]]; then
        print_info "Committing filed notes..."
        git add vault/ 2>/dev/null
        git commit -m "Process inbox: file notes to appropriate directories" --quiet 2>/dev/null
        git push origin main --quiet 2>/dev/null
    fi
    
    return 0
}

# Archive completed tasks in tasks.md
archive_completed_tasks() {
    cd "$(dirname "$0")/.." || return 1
    
    TASKS_FILE="vault/notes/tasks/tasks.md"
    
    # Check if tasks.md exists
    if [[ ! -f "$TASKS_FILE" ]]; then
        print_info "No tasks.md file found"
        return 0
    fi
    
    # Get today's date
    TODAY=$(date +%Y-%m-%d)
    
    # Create temporary files
    ACTIVE_TASKS=$(mktemp)
    COMPLETED_NEW=$(mktemp)
    ARCHIVE_CONTENT=$(mktemp)
    
    # Extract sections from the file
    # 1. Active section - preserve ALL structure (headers, tasks, blank lines) except completed tasks
    # This keeps project headers (## Project Name) intact
    awk '
        /^# Archive/ {exit}
        /^- \[x\]/ {next}  # Skip completed tasks
        {print}            # Keep everything else (headers, uncompleted tasks, blank lines)
    ' "$TASKS_FILE" | sed '/^$/N;/^\n$/d' > "$ACTIVE_TASKS" || true
    
    # 2. NEW completed tasks from active section (to be archived)
    awk '/^# Archive/ {exit} /^- \[x\]/ {print}' "$TASKS_FILE" > "$COMPLETED_NEW" || true
    
    # 3. Existing archive content (deduplicated)
    if grep -q "^# Archive" "$TASKS_FILE"; then
        awk '
            /^# Archive/ {in_archive=1; next}
            in_archive && /^## [0-9]{4}-[0-9]{2}-[0-9]{2}/ {
                current_date = substr($0, 4)
                print
                next
            }
            in_archive && /^- \[x\]/ {
                # Store task to check for duplicates
                task = $0
                if (!(task in seen)) {
                    seen[task] = 1
                    print
                }
                next
            }
            in_archive && /^- \[ \]/ {
                # Move incomplete tasks back to active
                next
            }
            in_archive {print}
        ' "$TASKS_FILE" > "$ARCHIVE_CONTENT" || true
    fi
    
    # Check if there are any new completed tasks to archive
    NEW_COUNT=$(wc -l < "$COMPLETED_NEW" | tr -d ' ')
    if [[ "$NEW_COUNT" -eq 0 ]]; then
        # No new completed tasks, but still rebuild to clean duplicates
        {
            # Active tasks
            if [[ -s "$ACTIVE_TASKS" ]]; then
                cat "$ACTIVE_TASKS"
            fi
            
            # Archive section
            if [[ -s "$ARCHIVE_CONTENT" ]] || grep -q "^# Archive" "$TASKS_FILE"; then
                echo ""
                echo "# Archive"
                echo ""
                if [[ -s "$ARCHIVE_CONTENT" ]]; then
                    cat "$ARCHIVE_CONTENT"
                fi
            fi
        } > "${TASKS_FILE}.tmp"
        
        mv "${TASKS_FILE}.tmp" "$TASKS_FILE"
        rm -f "$ACTIVE_TASKS" "$COMPLETED_NEW" "$ARCHIVE_CONTENT"
        
        print_info "No new completed tasks to archive, cleaned duplicates"
        return 0
    fi
    
    # Deduplicate new completed tasks
    sort -u "$COMPLETED_NEW" > "${COMPLETED_NEW}.dedup"
    mv "${COMPLETED_NEW}.dedup" "$COMPLETED_NEW"
    
    # Build the new file
    {
        # Active tasks first
        if [[ -s "$ACTIVE_TASKS" ]]; then
            cat "$ACTIVE_TASKS"
        fi
        
        # Archive section
        echo ""
        echo "# Archive"
        echo ""
        
        # Add today's section with new completed tasks
        echo "## $TODAY"
        cat "$COMPLETED_NEW"
        
        # Add existing archive content (if any)
        if [[ -s "$ARCHIVE_CONTENT" ]]; then
            echo ""
            cat "$ARCHIVE_CONTENT"
        fi
    } > "${TASKS_FILE}.tmp"
    
    # Replace the original file
    mv "${TASKS_FILE}.tmp" "$TASKS_FILE"
    
    # Clean up temp files
    rm -f "$ACTIVE_TASKS" "$COMPLETED_NEW" "$ARCHIVE_CONTENT"
    
    # Count unique completed tasks archived
    UNIQUE_COUNT=$(wc -l < "$COMPLETED_NEW" 2>/dev/null | tr -d ' ' || echo 0)
    print_info "Archived $UNIQUE_COUNT completed task(s) to $TODAY section"
    
    return 0
}

# Update cache statistics
update_cache_stats() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if cache database exists
    CACHE_DB=".data/today.db"
    if [[ ! -f "$CACHE_DB" ]]; then
        print_info "Cache database not found, initializing..."
        mkdir -p .data
        # Create an empty database with the required tables
        sqlite3 "$CACHE_DB" <<EOF
CREATE TABLE IF NOT EXISTS emails (
    id TEXT PRIMARY KEY,
    subject TEXT,
    from_address TEXT,
    date TEXT,
    has_been_replied_to INTEGER DEFAULT 0,
    text_content TEXT
);
CREATE TABLE IF NOT EXISTS task_cache (
    id TEXT PRIMARY KEY,
    title TEXT,
    due_date TEXT,
    stage TEXT,
    tags TEXT,
    description TEXT,
    last_edited_time TEXT
);
CREATE TABLE IF NOT EXISTS database_cache (
    id TEXT PRIMARY KEY,
    data TEXT
);
CREATE TABLE IF NOT EXISTS cache_metadata (
    key TEXT PRIMARY KEY,
    value TEXT
);
CREATE TABLE IF NOT EXISTS project_pillar_mapping (
    project_id TEXT,
    pillar_id TEXT
);
EOF
        print_info "Cache database initialized"
    fi
    
    # Get cache statistics
    cache_output=$(node -e "
        // Use environment variables already loaded by parent script
        import('./src/sqlite-cache.js').then(({default: DatabaseCache}) => {
            const cache = new DatabaseCache();
            const stats = cache.getStatistics();
            if (stats && stats.totalItems !== undefined) {
                console.log('Items: ' + stats.totalItems);
                console.log('Databases: ' + stats.totalDatabases);
                const sizeInMB = stats.cacheSize ? (stats.cacheSize / 1024 / 1024).toFixed(2) : '0.00';
                console.log('Size: ' + sizeInMB + ' MB');
            } else {
                console.log('Cache initialized');
            }
            cache.close();
        }).catch((err) => {
            console.error('Cache error: ' + err.message);
            process.exit(1);
        });
    " 2>&1)
    
    if [[ $? -eq 0 ]]; then
        if [[ -n "$cache_output" ]]; then
            while IFS= read -r line; do
                print_info "$line"
            done <<< "$cache_output"
        fi
        return 0
    else
        # Silently succeed - cache errors aren't critical
        return 0
    fi
}

# Parse command line arguments
case "${1:-}" in
    --vault)
        # Sync only GitHub vault and process inbox
        print_header "ðŸ“ GitHub Vault"
        if sync_github_vault; then
            print_status "Vault synced successfully!"
        else
            print_error "Notes sync failed"
        fi
        
        # Clean markdown files after syncing
        print_header "ðŸ§¹ Clean Markdown"
        if clean_markdown_files; then
            print_status "Markdown cleaned successfully!"
        else
            print_error "Markdown cleaning failed"
        fi
        
        # Also process inbox when syncing vault
        print_header "ðŸ“¥ Process Inbox"
        if process_inbox; then
            print_status "Inbox processed successfully!"
        else
            print_error "Inbox processing failed"
        fi
        
        # Archive completed tasks
        print_header "âœ… Archive Tasks"
        if archive_completed_tasks; then
            print_status "Tasks archived successfully!"
        fi
        ;;
    --quick)
        # Quick sync - only critical sources
        print_info "Quick sync mode - GitHub vault, tasks, inbox, and consistency check"
        sync_github_vault
        process_inbox
        SKIP_CLASSIFICATION=1 sync_tasks
        check_task_consistency
        ;;
    --force)
        # Force full sync - bypasses smart caching
        print_info "Force sync mode - refreshing all data"
        FORCE_SYNC=true
        main
        ;;
    --quick-email)
        # Just sync last day of email quickly
        print_header "ðŸ“§ Quick Email Sync"
        EMAIL_BIN=""
        if [[ -f bin/email ]]; then
            EMAIL_BIN="bin/email"
        elif [[ -f bin/email-cli ]]; then
            EMAIL_BIN="bin/email-cli"
        fi
        if [[ -n "$EMAIL_BIN" ]]; then
            print_info "Downloading emails from last 24 hours..."
            timeout 30 $EMAIL_BIN download --days 1 | grep -E "Downloaded|Total|âœ…" || true
            print_status "Quick email sync complete"
        else
            print_error "Email CLI not found"
        fi
        ;;
    --help|-h)
        echo "Usage: sync [OPTIONS]"
        echo ""
        echo "Intelligently synchronize all data sources"
        echo ""
        echo "Runs all operations synchronously for reliability:"
        echo "  â€¢ GitHub vault, task sync, markdown cleanup"
        echo "  â€¢ Task classification, email, calendar, contacts, Turso push"
        echo "  â€¢ Uses efficient timeouts and smart caching"
        echo ""
        echo "Options:"
        echo "  --vault       Sync GitHub vault only"
        echo "  --quick       Quick sync (GitHub and tasks only)"
        echo "  --force       Force full refresh (bypass cache)"
        echo "  --quick-email Download last 24 hours of email"
        echo "  --help        Show this help message"
        echo ""
        echo "Data sources synced:"
        echo "  â€¢ GitHub vault content"
        echo "  â€¢ Markdown formatting cleanup (via markdownlint)"
        echo "  â€¢ Local task manager"
        echo "  â€¢ Email inbox (cached for 4 hours)"
        echo "  â€¢ Calendar events"
        echo "  â€¢ iCloud contacts (cached for 4 hours)"
        echo "  â€¢ Inbox processing (files from Drafts)"
        echo "  â€¢ Local cache"
        ;;
    *)
        main
        ;;
esac