#!/bin/bash
# Unified sync script - Pull all external data sources
# This script synchronizes all data sources to prepare for AI review

set -e

# Load common dotenvx handler
source "$(dirname "$0")/lib/dotenvx-loader.sh"
auto_dotenvx "$@"

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}âœ“${NC} $1"
}

print_error() {
    echo -e "${RED}âœ—${NC} $1"
}

print_info() {
    echo -e "${BLUE}â„¹${NC} $1"
}

print_header() {
    echo ""
    echo -e "${YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${YELLOW}$1${NC}"
    echo -e "${YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
}

# Track what was synced
SYNC_SUMMARY=""

# Helper function to run a sync step with timing
run_sync_step() {
    local header="$1"
    local func="$2"
    local success_msg="$3"
    local summary_name="$4"
    
    print_header "$header"
    START_TIME=$(date +%s)
    if $func; then
        print_status "$success_msg"
        SYNC_SUMMARY="${SYNC_SUMMARY}âœ“ $summary_name\n"
    else
        print_error "$summary_name sync failed"
        SYNC_SUMMARY="${SYNC_SUMMARY}âœ— $summary_name\n"
    fi
    END_TIME=$(date +%s)
    print_info "â±ï¸  Took $((END_TIME - START_TIME)) seconds"
    # Always return 0 to prevent script exit with set -e
    return 0
}

# Main sync function
main() {
    echo ""
    echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${BLUE}â•‘       TODAY - Full Data Sync          â•‘${NC}"
    echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
    echo "Synchronizing all data sources..."
    echo "Started at: $(date '+%Y-%m-%d %H:%M:%S')"
    
    # Run all operations synchronously for reliability
    print_info "Running all sync operations synchronously..."
    
    # Critical path operations (fast, important)
    # Single bidirectional Turso sync (replaces separate pull and push)
    run_sync_step "â˜ï¸ Turso Sync" sync_turso_bidirectional "Turso synced" "Turso sync"
    # NOTE: Vault sync removed - now using Syncthing for real-time sync
    # run_sync_step "ðŸ“ GitHub Vault" sync_github_vault "GitHub vault synced" "GitHub vault"
    run_sync_step "ðŸ—‘ï¸ Cleanup Conflicts" cleanup_sync_conflicts "Sync conflicts cleaned" "Conflict cleanup"
    run_sync_step "ðŸ§¹ Clean Markdown" clean_markdown_files "Markdown cleaned" "Markdown cleanup"
    run_sync_step "ðŸ—„ï¸ Clean Database" clean_database_task_titles "Database cleaned" "Database cleanup"
    
    # Task sync without classification
    SKIP_CLASSIFICATION=1 run_sync_step "âœ… Task Manager" sync_tasks "Tasks synced" "Task manager"
    run_sync_step "ðŸ“¥ Process Inbox" process_inbox "Inbox processed" "Inbox processed"
    run_sync_step "âœ… Archive Tasks" archive_completed_tasks "Tasks archived" "Tasks archived"
    run_sync_step "ðŸ“ Sync Project Tasks" sync_project_tasks "Project tasks synced" "Project tasks"
    
    # Run additional operations synchronously (no background jobs)
    print_header "âš¡ Additional Sync Operations"
    
    # Task classification in background (it's slow and non-critical)
    print_info "ðŸ” Starting task classification in background..."
    {
        # Run stage classification
        echo "Starting stage classification..." > /tmp/classify-$$.log
        bin/tasks classify-stages >> /tmp/classify-$$.log 2>&1
        if [ $? -eq 0 ]; then
            echo "âœ… Stage classification completed successfully" >> /tmp/classify-$$.log
        else
            echo "âŒ Stage classification failed" >> /tmp/classify-$$.log
        fi
        
        # Run topic assignment
        echo "" >> /tmp/classify-$$.log
        echo "Starting topic assignment..." >> /tmp/classify-$$.log
        bin/tasks add-topics >> /tmp/classify-$$.log 2>&1
        if [ $? -eq 0 ]; then
            echo "âœ… Topic assignment completed successfully" >> /tmp/classify-$$.log
        else
            echo "âŒ Topic assignment failed" >> /tmp/classify-$$.log
        fi
        
        # Run status prioritization
        echo "" >> /tmp/classify-$$.log
        echo "Starting status prioritization..." >> /tmp/classify-$$.log
        bin/tasks prioritize-status >> /tmp/classify-$$.log 2>&1
        if [ $? -eq 0 ]; then
            echo "âœ… Status prioritization completed successfully" >> /tmp/classify-$$.log
        else
            echo "âŒ Status prioritization failed" >> /tmp/classify-$$.log
        fi
    } &
    CLASSIFY_PID=$!
    print_info "   Classification, topics & status prioritization running in background (PID: $CLASSIFY_PID)"
    print_info "   Check progress: tail -f /tmp/classify-$$.log"
    
    # Email sync (if configured)
    if grep -q "EMAIL_ACCOUNT=" .env 2>/dev/null && grep -q "EMAIL_PASSWORD=" .env 2>/dev/null; then
        run_sync_step "ðŸ“§ Email Sync" sync_email "Email synced" "Email sync"
    fi
    
    # Pobox sent email sync (if configured)
    if grep -q "POBOX_ACCOUNT=" .env 2>/dev/null && grep -q "POBOX_PASSWORD=" .env 2>/dev/null; then
        run_sync_step "ðŸ“¤ Pobox Sent Mail" sync_pobox "Sent emails synced" "Pobox sync"
    fi
    
    # Calendar sync
    run_sync_step "ðŸ“… Calendar Sync" sync_calendar "Calendar synced" "Calendar sync"
    
    # Contacts sync
    run_sync_step "ðŸ‘¥ Contacts Sync" sync_contacts "Contacts synced" "Contacts sync"
    
    # Contact tracking sync (bidirectional between database and every_six_weeks.md)
    run_sync_step "ðŸ“ž Contact Tracking" sync_contact_tracking "Contact tracking synced" "Contact tracking"
    
    # Day One diary sync (if Journal.json exists)
    if [[ -f vault/logs/Journal.json ]]; then
        run_sync_step "ðŸ“” Day One Diary" sync_diary "Diary synced" "Day One diary"
    fi
    
    # Toggl time tracking sync (if configured)
    if grep -q "TOGGL_API_TOKEN=" .env 2>/dev/null && ! grep -q "TOGGL_API_TOKEN=$" .env 2>/dev/null; then
        run_sync_step "â±ï¸ Toggl Sync" sync_toggl "Toggl synced" "Toggl sync"
    fi
    
    # OlderGay.Men monitoring sync (if configured)
    if grep -q "GITHUB_ACCESS_TOKEN=" .env 2>/dev/null || grep -q "HONEYBADGER_AUTH_TOKEN=" .env 2>/dev/null || grep -q "SCOUT_API_KEY=" .env 2>/dev/null; then
        run_sync_step "ðŸŒ OGM Monitoring" sync_ogm_monitoring "OGM data synced" "OGM monitoring"
    fi
    
    # NOTE: Final vault push removed - Syncthing handles vault sync now
    # run_sync_step "ðŸ“ Final Vault Push" sync_github_vault "Vault changes pushed" "Final vault push"
    
    print_status "All sync operations completed (classification running in background)."
    
    # Print summary
    print_header "ðŸ“‹ Sync Summary"
    echo -e "$SYNC_SUMMARY"
    echo "Completed at: $(date '+%Y-%m-%d %H:%M:%S')"
    echo ""
    
    # Check if any sync failed
    if echo -e "$SYNC_SUMMARY" | grep -q "âœ—"; then
        print_error "Some data sources failed to sync"
        echo "Please configure missing services in .env file"
        exit 1
    else
        print_status "All data sources synchronized!"
    fi
    
    echo ""
    if [ ! -z "$CLASSIFY_PID" ]; then
        echo "Note: Task stage & topic classification is still running in background (PID: $CLASSIFY_PID)"
        echo "      Check progress: tail -f /tmp/classify-$$.log"
    fi
    echo ""
    echo "Next step: Run 'bin/today' to get AI suggestions on what to do today"
}

# Sync GitHub vault
sync_github_vault() {
    # DEPRECATED: Vault is now synced via Syncthing, not GitHub
    # This function is kept for backwards compatibility but does nothing
    print_info "Vault sync skipped - using Syncthing for real-time sync"
    return 0
}

sync_github_vault_original() {
    # Original implementation kept for reference
    # Change to project directory
    cd "$(dirname "$0")/.." || return 1
    
    # ALWAYS pull first to get latest changes before trying to push
    # This prevents push failures when remote has changes
    print_info "Fetching latest changes from GitHub..."
    git fetch origin main --quiet 2>/dev/null
    
    # Check if we're behind and need to pull
    LOCAL=$(git rev-parse HEAD)
    REMOTE=$(git rev-parse origin/main)
    
    if [[ "$LOCAL" != "$REMOTE" ]]; then
        # We're behind - need to handle any local changes first
        if [[ -n $(git status --porcelain 2>/dev/null) ]]; then
            print_info "Stashing local changes before pull..."
            git stash push --quiet -m "sync-auto-stash-pre-pull" 2>/dev/null
            PRE_PULL_STASH=true
        fi
        
        print_info "Pulling latest changes from GitHub..."
        if git pull origin main --quiet --no-edit 2>/dev/null; then
            print_status "Pulled latest changes from GitHub"
        else
            print_error "Pull failed - manual resolution needed"
            return 1
        fi
        
        # Restore stashed changes
        if [[ "${PRE_PULL_STASH:-false}" == "true" ]]; then
            print_info "Restoring local changes..."
            git stash pop --quiet 2>/dev/null || true
        fi
    fi
    
    # Now check if we have local changes to commit and push
    if [[ -n $(git status --porcelain 2>/dev/null) ]]; then
        # Check if there are changes ONLY in vault/ (no other staged or unstaged changes)
        CONTENT_CHANGES=$(git status vault/ --porcelain 2>/dev/null)
        OTHER_CHANGES=$(git status --porcelain 2>/dev/null | grep -v "^.. vault/" || true)
        
        if [[ -n "$CONTENT_CHANGES" ]] && [[ -z "$OTHER_CHANGES" ]]; then
            # Only content changes, safe to commit
            print_info "Pushing local content changes..."
            git add vault/ 2>/dev/null
            git commit -m "Sync vault content" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
        elif [[ -n "$CONTENT_CHANGES" ]]; then
            # We have both content changes and other changes
            # First, stash any staged changes outside of vault/
            if [[ -n $(git diff --cached --name-only | grep -v "^vault/" || true) ]]; then
                print_info "Saving staged changes outside vault..."
                # Create a temporary commit to preserve staged state
                git commit --quiet -m "temp: staged changes" -- ':!vault/' 2>/dev/null || true
                TEMP_COMMIT=true
            fi
            
            # Now commit just the content
            print_info "Pushing local content changes..."
            git add vault/ 2>/dev/null
            git commit -m "Sync vault content" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
            
            # Restore the temporary commit if we made one
            if [[ "${TEMP_COMMIT:-false}" == "true" ]]; then
                print_info "Restoring staged changes..."
                git reset --soft HEAD~1 2>/dev/null
            fi
        else
            # Changes in other files but not content - stash them temporarily
            print_info "Stashing local changes outside content directories..."
            git stash push --quiet -m "sync-auto-stash" -- ':!vault/' 2>/dev/null
            STASHED=true
        fi
    fi
    
    # Restore stashed changes if we stashed them
    if [[ "${STASHED:-false}" == "true" ]]; then
        print_info "Restoring stashed changes..."
        git stash pop --quiet 2>/dev/null || true
    fi
    
    # Clean up empty files in all content directories
    find vault -type f -empty -delete 2>/dev/null
    deleted_count=$(git status --porcelain vault/ | grep -c "^ D " || true)
    if [[ $deleted_count -gt 0 ]]; then
        print_info "Removed $deleted_count empty file(s)"
        git add vault/
        git commit -m "Clean up empty vault files" --quiet 2>/dev/null
        git push origin main --quiet 2>/dev/null
    fi
    
    # Show content counts
    note_count=$(find vault/notes -type f -name "*.md" 2>/dev/null | wc -l)
    plan_count=$(find vault/plans -type f -name "*.md" 2>/dev/null | wc -l)
    project_count=$(find vault/projects -type f -name "*.md" 2>/dev/null | wc -l)
    topic_count=$(find vault/topics -type f -name "*.md" 2>/dev/null | wc -l)
    print_info "Total notes: $note_count, plans: $plan_count, projects: $project_count, topics: $topic_count"
    
    # Count OGM work files
    ogm_count=$(find vault/logs/ogm-work -type f -name "*.md" 2>/dev/null | wc -l || echo 0)
    if [[ $ogm_count -gt 0 ]]; then
        print_info "OlderGay.Men work summaries: $ogm_count"
    fi
    
    # Show recent content (last 7 days) across all directories
    recent_notes=$(find vault/notes -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_plans=$(find vault/plans -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_projects=$(find vault/projects -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_topics=$(find vault/topics -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    recent_count=$((recent_notes + recent_plans + recent_projects + recent_topics))
    
    if [[ $recent_count -gt 0 ]]; then
        print_info "Content from last 7 days: $recent_count files (notes: $recent_notes, plans: $recent_plans, projects: $recent_projects, topics: $recent_topics)"
        # Show up to 5 most recent files across all directories
        print_info "Most recent content:"
        find vault -type f -name "*.md" -mtime -7 2>/dev/null | \
            xargs ls -t 2>/dev/null | head -5 | while read -r file; do
            echo "  â€¢ $(basename "$file") ($(dirname "$file"))"
        done
    fi
    
    return 0
}

# Clean up Syncthing sync-conflict files
cleanup_sync_conflicts() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Checking for Syncthing sync-conflict files..."
    
    # Find all sync-conflict files
    CONFLICT_FILES=$(find vault -name "*sync-conflict-*" -type f 2>/dev/null)
    
    if [[ -z "$CONFLICT_FILES" ]]; then
        print_info "No sync-conflict files found"
        return 0
    fi
    
    TOTAL_CONFLICTS=$(echo "$CONFLICT_FILES" | wc -l)
    print_info "Found $TOTAL_CONFLICTS sync-conflict file(s)"
    
    REMOVED_COUNT=0
    KEPT_COUNT=0
    
    # Process each conflict file
    while IFS= read -r conflict_file; do
        if [[ -z "$conflict_file" ]]; then
            continue
        fi
        
        # Extract the original filename by removing the sync-conflict suffix
        # Pattern: filename.sync-conflict-YYYYMMDD-HHMMSS-XXXXX.extension
        original_file=$(echo "$conflict_file" | sed 's/\.sync-conflict-[0-9]\{8\}-[0-9]\{6\}-[A-Z0-9]\{7\}//')
        
        if [[ -f "$original_file" ]]; then
            # Both files exist, compare them
            if diff -q "$original_file" "$conflict_file" > /dev/null 2>&1; then
                # Files are identical, safe to remove conflict
                rm "$conflict_file"
                REMOVED_COUNT=$((REMOVED_COUNT + 1))
                print_info "  âœ“ Removed duplicate: $(basename "$conflict_file")"
            else
                # Files differ, check which is newer
                # Use stat -c for Linux (GitHub Codespaces uses Linux)
                original_mtime=$(stat -c %Y "$original_file" 2>/dev/null)
                conflict_mtime=$(stat -c %Y "$conflict_file" 2>/dev/null)
                
                if [[ "$conflict_mtime" -lt "$original_mtime" ]]; then
                    # Conflict file is older than original, safe to remove
                    rm "$conflict_file"
                    REMOVED_COUNT=$((REMOVED_COUNT + 1))
                    print_info "  âœ“ Removed older conflict: $(basename "$conflict_file")"
                else
                    # Conflict file is newer or same age, keep it for manual review
                    KEPT_COUNT=$((KEPT_COUNT + 1))
                    # print_info "  âš  Keeping for review: $(basename "$conflict_file")"
                fi
            fi
        else
            # Original file doesn't exist, rename conflict to original
            mv "$conflict_file" "$original_file"
            REMOVED_COUNT=$((REMOVED_COUNT + 1))
            print_info "  âœ“ Restored missing file: $(basename "$original_file")"
        fi
    done <<< "$CONFLICT_FILES"
    
    if [[ $REMOVED_COUNT -gt 0 ]]; then
        print_info "Cleaned up $REMOVED_COUNT sync-conflict file(s)"
    fi
    
    if [[ $KEPT_COUNT -gt 0 ]]; then
        print_info "Kept $KEPT_COUNT conflict file(s) for manual review (newer than original)"
    fi
    
    return 0
}

# Clean markdown files with markdownlint
clean_markdown_files() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Cleaning markdown files with markdownlint..."
    
    # Track last lint run using a marker file
    LINT_MARKER=".data/.last-markdown-lint"
    mkdir -p .data 2>/dev/null
    
    # Save current timestamp BEFORE we start any processing
    # We'll use this to mark when we started, so files we modify don't get reprocessed
    TEMP_MARKER=$(mktemp)
    touch "$TEMP_MARKER"
    
    # Find files changed since last lint run
    if [[ -f "$LINT_MARKER" ]]; then
        # Files modified after the last lint run
        ALL_CHANGED_FILES=$(find vault -type f -name "*.md" -newer "$LINT_MARKER" -not -path "*/.*" -not -name "*sync-conflict*" 2>/dev/null || true)
    else
        # First run or marker missing - process files from last 5 minutes
        ALL_CHANGED_FILES=$(find vault -type f -name "*.md" -mmin -5 -not -path "*/.*" -not -name "*sync-conflict*" 2>/dev/null || true)
    fi
    
    # Count changed files
    if [[ -z "$ALL_CHANGED_FILES" ]]; then
        print_info "No markdown files changed since last lint"
        # Update marker with the time we checked
        mv "$TEMP_MARKER" "$LINT_MARKER"
        return 0
    fi
    
    MD_COUNT=$(echo "$ALL_CHANGED_FILES" | wc -l | tr -d ' ')
    print_info "Processing $MD_COUNT markdown file(s) changed since last lint..."
    
    # First, fix corrupted HTML comments (smart dash replacements from Drafts, etc.)
    # This fixes: <!â€” becomes <!-- and â€”> becomes -->
    COMMENT_FIXED_COUNT=0
    while IFS= read -r file; do
        # Skip empty lines
        [[ -z "$file" ]] && continue
        # Check if file has corrupted comments
        if grep -qE '<!â€”|â€”>' "$file" 2>/dev/null; then
            # Fix the corrupted comments
            # Using sed with in-place editing
            if [[ "$(uname)" == "Darwin" ]]; then
                # macOS version
                sed -i '' -e 's/<!â€”/<!--/g' -e 's/â€”>/-->/g' "$file"
            else
                # Linux version
                sed -i -e 's/<!â€”/<!--/g' -e 's/â€”>/-->/g' "$file"
            fi
            COMMENT_FIXED_COUNT=$((COMMENT_FIXED_COUNT + 1))
        fi
    done <<< "$ALL_CHANGED_FILES"
    
    if [[ $COMMENT_FIXED_COUNT -gt 0 ]]; then
        print_info "Fixed corrupted HTML comments in $COMMENT_FIXED_COUNT file(s)"
    fi
    
    # Fix malformed task-id tags (multiple task-ids on same line)
    # This handles cases like: <!-- task-id: abc <!-- task-id: def -->
    TASK_ID_FIXED_COUNT=0
    while IFS= read -r file; do
        # Skip empty lines
        [[ -z "$file" ]] && continue
        # Check if file has malformed task-id tags (multiple on same line)
        # This matches both incomplete (<!-- task-id:.*<!-- task-id:) and complete (<!-- task-id: .* --> <!-- task-id:)
        if grep -qE '<!-- task-id:.*<!-- task-id:' "$file" 2>/dev/null || grep -qE '<!-- task-id: [^>]+ --> <!-- task-id:' "$file" 2>/dev/null; then
            # Create temp file
            TEMP_FILE=$(mktemp)
            
            # Process each line to fix malformed task-ids
            while IFS= read -r line; do
                # Check if line has multiple task-id tags (complete or incomplete)
                if echo "$line" | grep -qE '<!-- task-id:.*<!-- task-id:' || echo "$line" | grep -qE '<!-- task-id: [^>]+ --> <!-- task-id:'; then
                    # Extract the LAST valid hex task-id value (32 character hex string, with or without dashes)
                    # Only match valid hex patterns: either 32 chars without dashes or 36 chars with proper dash placement
                    # First try to get IDs with dashes (36 chars total: 8-4-4-4-12 pattern)
                    LAST_TASK_ID=$(echo "$line" | grep -oE 'task-id: [a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}' | tail -1 | sed 's/task-id: //')
                    # If no dashed ID found, try 32-char format without dashes
                    if [[ -z "$LAST_TASK_ID" ]]; then
                        LAST_TASK_ID=$(echo "$line" | grep -oE 'task-id: [a-f0-9]{32}' | tail -1 | sed 's/task-id: //')
                    fi
                    
                    if [[ -n "$LAST_TASK_ID" ]]; then
                        # Remove ALL task-id related HTML comments (complete or incomplete)
                        # This handles both <!-- task-id: xxx --> and <!-- task-id: xxx (incomplete)
                        # Remove any task-id comment (with any content between task-id: and -->)
                        CLEANED_LINE=$(echo "$line" | sed -E 's/<!-- task-id: [^>]* -->//g' | sed 's/[[:space:]]*$//')
                        # Add back a single, properly formatted task-id
                        echo "${CLEANED_LINE} <!-- task-id: ${LAST_TASK_ID} -->" >> "$TEMP_FILE"
                    else
                        # No valid task-id found, remove ALL task-id tags (including invalid ones)
                        # This removes any <!-- task-id: ... --> pattern
                        echo "$line" | sed -E 's/<!-- task-id: [^>]* -->//g' | sed 's/[[:space:]]*$//' >> "$TEMP_FILE"
                    fi
                else
                    echo "$line" >> "$TEMP_FILE"
                fi
            done < "$file"
            
            # Replace original file
            mv "$TEMP_FILE" "$file"
            TASK_ID_FIXED_COUNT=$((TASK_ID_FIXED_COUNT + 1))
        fi
    done <<< "$ALL_CHANGED_FILES"
    
    if [[ $TASK_ID_FIXED_COUNT -gt 0 ]]; then
        print_info "Fixed malformed task-id tags in $TASK_ID_FIXED_COUNT file(s)"
    fi
    
    # Run markdownlint-cli2 with fix option on changed files only
    # Convert the list of files to arguments for markdownlint
    if [[ -n "$ALL_CHANGED_FILES" ]]; then
        # Convert newline-separated list to space-separated arguments
        FILES_TO_LINT=$(echo "$ALL_CHANGED_FILES" | tr '\n' ' ')
        output=$(npx markdownlint-cli2 --fix $FILES_TO_LINT 2>&1) || true
    else
        output=""
    fi
    
    # Check if any files were fixed
    if echo "$output" | grep -q "Fixing:"; then
        fixed_count=$(echo "$output" | grep -c "Fixing:" || echo 0)
        print_info "Fixed formatting in $fixed_count file(s)"
    else
        print_info "All markdown files are already clean"
    fi
    
    # Update the marker file with the timestamp from BEFORE we started processing
    # This ensures files we just modified won't be re-processed next time
    mv "$TEMP_MARKER" "$LINT_MARKER"
    
    # NOTE: No longer committing vault changes - Syncthing handles synchronization
    # Files are cleaned in-place and will sync automatically
    
    return 0
}

# Clean malformed task titles in database
clean_database_task_titles() {
    cd "$(dirname "$0")/.." || return 1
    
    # Find and fix task titles that contain incomplete task-id comments
    # This happens when tasks get duplicated with malformed titles
    
    # Count affected tasks
    MALFORMED_COUNT=$(sqlite3 .data/today.db "SELECT COUNT(*) FROM tasks WHERE title LIKE '%<!-- task-id:%' AND title NOT LIKE '%-->';" 2>/dev/null || echo "0")
    
    if [[ "$MALFORMED_COUNT" -gt 0 ]]; then
        print_info "Fixing $MALFORMED_COUNT task(s) with malformed task-id in title..."
        
        # Remove incomplete task-id comments from titles
        sqlite3 .data/today.db "UPDATE tasks SET title = SUBSTR(title, 1, INSTR(title, ' <!-- task-id:') - 1) WHERE title LIKE '%<!-- task-id:%' AND title NOT LIKE '%-->';" 2>/dev/null || true
        
        print_info "Fixed malformed task titles in database"
    fi
    
    return 0
}

# Check task consistency across all markdown files
check_task_consistency() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Checking task consistency across markdown files..."
    
    # Run consistency check with auto-fix enabled
    # This now works correctly because:
    # 1. We include vault/tasks/ files to respect manual changes there
    # 2. We use file modification time to determine the correct state
    # 3. The latest change wins, whether it's in source or generated files
    output=$(bin/tasks check-consistency --fix 2>&1)
    
    if echo "$output" | grep -q "All tasks are consistent"; then
        print_info "All tasks are consistent"
        return 0
    elif echo "$output" | grep -q "Fixed.*inconsistent task"; then
        # Extract the number of fixed tasks
        fixed_count=$(echo "$output" | grep -oE "Fixed [0-9]+ inconsistent" | grep -oE "[0-9]+")
        print_info "Fixed $fixed_count inconsistent task(s)"
        return 0
    else
        print_error "Task consistency check failed"
        return 1
    fi
}

# Sync task manager (includes stage classification)
sync_tasks() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Syncing task database with markdown files..."
    
    # Run the task sync command (now includes stage classification)
    output=$(timeout 30 bin/tasks sync 2>&1)
    
    if [[ $? -eq 0 ]]; then
        # Show the sync output but filter for key info
        echo "$output" | while IFS= read -r line; do
            # Highlight stage classification results
            if echo "$line" | grep -qE "Classified|Stage|Front|Back|Off"; then
                print_info "$line"
            elif echo "$line" | grep -q "âœ“"; then
                print_info "$line"
            elif echo "$line" | grep -q "Generated today.md"; then
                print_info "$line"
            fi
        done
        return 0
    else
        print_error "Task sync failed"
        echo "$output" | while IFS= read -r line; do
            print_info "$line"
        done
        return 1
    fi
}

# Bidirectional sync with Turso cloud database
sync_turso_bidirectional() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if Turso is configured
    if [[ -z "$TURSO_DATABASE_URL" ]] || [[ -z "$TURSO_AUTH_TOKEN" ]]; then
        print_info "Turso not configured (need TURSO_DATABASE_URL and TURSO_AUTH_TOKEN)"
        return 1
    fi
    
    # Use the turso-sync script with sync action for bidirectional sync
    print_info "Performing bidirectional sync with Turso..."
    
    # Run the turso-sync script with sync action (newest data wins)
    if timeout 45 bin/turso-sync sync 2>&1 | while IFS= read -r line; do
        if [[ "$line" == *"âœ…"* ]] || [[ "$line" == *"âœ“"* ]] || [[ "$line" == *"Bidirectional sync complete"* ]]; then
            print_info "$line"
        elif [[ "$line" == *"âœ—"* ]] || [[ "$line" == *"Error"* ]] || [[ "$line" == *"Failed"* ]]; then
            print_error "$line"
        elif [[ "$line" == *"Comparing timestamps"* ]] || [[ "$line" == *"Step"* ]]; then
            print_info "$line"
        fi
    done; then
        return 0
    else
        local exit_code=$?
        if [[ $exit_code -eq 124 ]]; then
            print_info "Turso sync timed out after 45s - continuing with local data"
            return 0  # Don't fail the sync, just continue
        else
            print_error "Turso sync failed"
            return 1
        fi
    fi
}

# Sync Notion database
sync_notion() {
    # Use the notion CLI to refresh cache
    cd "$(dirname "$0")/.." || return 1
    
    # Check if NOTION_TOKEN is configured
    if ! grep -q "NOTION_TOKEN=" .env 2>/dev/null || grep -q "NOTION_TOKEN=your_notion" .env 2>/dev/null; then
        print_error "Notion not configured"
        print_info "Add NOTION_TOKEN to .env file"
        return 1
    fi
    
    # Set up cache database
    CACHE_DB=".data/today.db"
    
    # Ensure sync_metadata table exists
    if [[ -f "$CACHE_DB" ]]; then
        sqlite3 "$CACHE_DB" "CREATE TABLE IF NOT EXISTS sync_metadata (key TEXT PRIMARY KEY, value TEXT)" 2>/dev/null
        LAST_SYNC=$(sqlite3 "$CACHE_DB" "SELECT value FROM sync_metadata WHERE key='last_full_sync'" 2>/dev/null || echo "")
    else
        print_info "No cache found, initial sync needed..."
        LAST_SYNC=""
    fi
    
    # For now, use the regular fetch-tasks which is fast when cached
    print_info "Syncing Notion tasks..."
    
    # Fetch tasks (will use cache intelligently)
    task_output=$(bin/notion fetch-tasks 2>&1 | tail -5)
    if [[ $? -eq 0 ]]; then
        echo "$task_output" | while IFS= read -r line; do
            print_info "$line"
        done
        
        # Update sync timestamp
        sqlite3 "$CACHE_DB" "INSERT OR REPLACE INTO sync_metadata (key, value) VALUES ('last_full_sync', datetime('now'))" 2>/dev/null
        
        return 0
    else
        print_error "Notion connection failed"
        if echo "$output" | grep -q "NOTION_TOKEN"; then
            print_info "Please configure NOTION_TOKEN in .env file"
        else
            print_info "Error: $output"
        fi
        return 1
    fi
}

# Sync email
sync_email() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if email-cli/email binary exists
    EMAIL_BIN=""
    if [[ -f bin/email ]]; then
        EMAIL_BIN="bin/email"
    elif [[ -f bin/email-cli ]]; then
        EMAIL_BIN="bin/email-cli"
    else
        print_error "Email CLI not available"
        return 1
    fi
    
    # Check if email credentials are configured
    if ! grep -q "EMAIL_ACCOUNT=" .env 2>/dev/null || ! grep -q "EMAIL_PASSWORD=" .env 2>/dev/null; then
        print_error "Email not configured"
        print_info "Run 'email setup' for configuration instructions"
        return 1
    fi
    
    # Incremental email sync - fetch emails newer than what we have
    if [[ -f .data/today.db ]]; then
        # Get the timestamp of the most recent email
        EMAIL_INFO=$(sqlite3 .data/today.db "SELECT MAX(date), COUNT(*) FROM emails" 2>/dev/null || echo "||0")
        LAST_EMAIL_DATE=$(echo "$EMAIL_INFO" | cut -d'|' -f1)
        EMAIL_COUNT=$(echo "$EMAIL_INFO" | cut -d'|' -f2)
        
        if [[ -n "$LAST_EMAIL_DATE" ]] && [[ "$EMAIL_COUNT" -gt 0 ]]; then
            # Calculate days since last email to determine sync range
            DAYS_OLD=$(node -e "
                const date = new Date('$LAST_EMAIL_DATE');
                const now = new Date();
                const daysSince = (now - date) / (1000 * 60 * 60 * 24);
                console.log(Math.ceil(daysSince));
            " 2>/dev/null || echo "7")
            
            # Sync enough days to catch all new emails since last sync
            # Add 1 day buffer to ensure we don't miss anything
            DAYS_TO_SYNC=$((DAYS_OLD + 1))
            
            # Cap at 7 days max for performance
            if [[ $DAYS_TO_SYNC -gt 7 ]]; then
                DAYS_TO_SYNC=7
                print_info "Syncing emails from last 7 days (max range)..."
            else
                print_info "Syncing emails from last $DAYS_TO_SYNC days to catch all new mail..."
            fi
        else
            print_info "No emails in database, initial sync of 7 days..."
            DAYS_TO_SYNC=7
        fi
    else
        print_info "No email database found, initial sync of 7 days..."
        DAYS_TO_SYNC=7
    fi
    
    # Use timeout based on days with 30 second base
    TIMEOUT=$((30 + $DAYS_TO_SYNC * 5))  # 30s base + 5s per day
    print_info "Fetching new emails (timeout: ${TIMEOUT}s)..."
    
    # Create a temp file for output
    EMAIL_OUTPUT=$(mktemp)
    
    # Run email download in background with timeout, showing progress
    timeout $TIMEOUT $EMAIL_BIN download --days $DAYS_TO_SYNC > "$EMAIL_OUTPUT" 2>&1 &
    EMAIL_PID=$!
    
    # Monitor the output file for "already in progress" or transaction error messages
    WAIT_COUNT=0
    while kill -0 $EMAIL_PID 2>/dev/null; do
        if grep -q "Another download is already in progress" "$EMAIL_OUTPUT" 2>/dev/null; then
            print_info "Another email download is already in progress, skipping..."
            kill $EMAIL_PID 2>/dev/null || true
            wait $EMAIL_PID 2>/dev/null || true
            rm -f "$EMAIL_OUTPUT"
            return 0
        fi
        
        # Check for database transaction errors
        if grep -q "this.cache.db.transaction is not a function" "$EMAIL_OUTPUT" 2>/dev/null; then
            print_error "Email database error detected"
            print_info "Try running 'bin/email repair' to fix the database"
            kill $EMAIL_PID 2>/dev/null || true
            wait $EMAIL_PID 2>/dev/null || true
            rm -f "$EMAIL_OUTPUT"
            return 1
        fi
        
        # Show any new output lines (for progress)
        if [[ -f "$EMAIL_OUTPUT" ]] && [[ -s "$EMAIL_OUTPUT" ]]; then
            tail -n +$((WAIT_COUNT + 1)) "$EMAIL_OUTPUT" 2>/dev/null | while IFS= read -r line; do
                if [[ -n "$line" ]]; then
                    echo "  $line"
                fi
            done
            WAIT_COUNT=$(wc -l < "$EMAIL_OUTPUT" 2>/dev/null || echo 0)
        fi
        
        sleep 0.5
    done
    
    # Get exit code (don't wait forever if process was killed)
    wait $EMAIL_PID 2>/dev/null || true
    email_exit_code=$?
    
    # Read final output
    output=$(cat "$EMAIL_OUTPUT" 2>/dev/null || echo "")
    rm -f "$EMAIL_OUTPUT"
    
    # Check for timeout
    if [[ $email_exit_code -eq 124 ]]; then
        print_error "Email download timed out after $TIMEOUT seconds"
        print_info "Try running 'email download --days 1' for a quicker sync"
        return 1
    elif [[ $email_exit_code -ne 0 ]]; then
        print_error "Email download failed"
        return 1
    fi
    
    # Show download results
    if echo "$output" | grep -q "Downloaded"; then
        # Extract number of emails downloaded
        email_count=$(echo "$output" | grep -oE "Downloaded [0-9]+ email" | grep -oE "[0-9]+") || true
        if [[ -n "$email_count" ]]; then
            print_info "Downloaded $email_count new emails"
        fi
    else
        print_info "No new emails to download"
    fi
    
    # Show email stats (with timeout to prevent hanging)
    stats_output=$(timeout 2 $EMAIL_BIN stats 2>&1 | grep "Total emails:" | cut -d: -f2 | xargs) || true
    if [[ -n "$stats_output" ]]; then
        print_info "Total emails in database: $stats_output"
    fi
    
    return 0
}

# Sync Pobox sent emails
sync_pobox() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if Pobox credentials are configured
    if ! grep -q "POBOX_ACCOUNT=" .env 2>/dev/null || ! grep -q "POBOX_PASSWORD=" .env 2>/dev/null; then
        print_info "Pobox credentials not configured (skipping)"
        return 0
    fi
    
    print_info "Downloading sent emails from Pobox..."
    
    # Create temp file for output
    POBOX_OUTPUT=$(mktemp /tmp/pobox-sync.XXXXXX)
    
    # Start the sync with timeout
    TIMEOUT=60
    timeout $TIMEOUT bin/pobox-sync > "$POBOX_OUTPUT" 2>&1 &
    POBOX_PID=$!
    
    # Show progress dots while running
    local dots=""
    while kill -0 $POBOX_PID 2>/dev/null; do
        dots="${dots}."
        if [[ ${#dots} -gt 30 ]]; then
            dots=""
        fi
        printf "\r   Syncing sent emails%-35s" "$dots"
        sleep 0.5
    done
    
    # Get exit code
    wait $POBOX_PID 2>/dev/null || true
    pobox_exit_code=$?
    
    # Clear the progress line
    printf "\r%-70s\r" " "
    
    # Read output
    output=$(cat "$POBOX_OUTPUT" 2>/dev/null || echo "")
    rm -f "$POBOX_OUTPUT"
    
    # Check for timeout
    if [[ $pobox_exit_code -eq 124 ]]; then
        print_error "Pobox sync timed out after $TIMEOUT seconds"
        return 1
    elif [[ $pobox_exit_code -ne 0 ]]; then
        print_error "Pobox sync failed"
        echo "$output" | grep -v "^$" | head -5
        return 1
    fi
    
    # Show results
    if echo "$output" | grep -q "Downloaded"; then
        # Extract number of emails
        email_count=$(echo "$output" | grep -oE "Downloaded [0-9]+ sent email" | grep -oE "[0-9]+") || true
        if [[ -n "$email_count" ]]; then
            print_info "Downloaded $email_count new sent emails"
        fi
    else
        print_info "No new sent emails to download"
    fi
    
    return 0
}

# Sync calendar events
sync_calendar() {
    cd "$(dirname "$0")/.." || return 1
    
    # Run calendar sync
    print_info "Syncing calendar events..."
    SYNC_OUTPUT=$(bin/calendar sync 2>&1)
    
    # Check if sync was successful
    if echo "$SYNC_OUTPUT" | grep -q "âœ… Synced [0-9]"; then
        # Extract event count
        EVENT_COUNT=$(echo "$SYNC_OUTPUT" | grep -oE "âœ… Synced [0-9]+ calendar" | grep -oE "[0-9]+" || echo 0)
        if [[ "$EVENT_COUNT" -gt 0 ]]; then
            print_info "Synced $EVENT_COUNT calendar events"
            # Show quick summary of today's events
            TODAY_COUNT=$(bin/calendar today 2>/dev/null | jq '. | length' 2>/dev/null || echo 0)
            if [[ "$TODAY_COUNT" -gt 0 ]]; then
                print_info "$TODAY_COUNT events scheduled for today"
            fi
            return 0
        else
            print_info "No calendar events found"
            return 0
        fi
    else
        print_info "No calendars configured (run 'bin/setup --calendar' for instructions)"
        return 1
    fi
}

# Sync Toggl time tracking
sync_toggl() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if toggl binary exists
    if [[ ! -f bin/toggl ]]; then
        print_error "Toggl CLI not available"
        return 1
    fi
    
    # Check if Toggl API token is configured
    if ! grep -q "TOGGL_API_TOKEN=" .env 2>/dev/null || grep -q "TOGGL_API_TOKEN=$" .env 2>/dev/null; then
        print_info "Toggl not configured (need TOGGL_API_TOKEN)"
        return 1
    fi
    
    # Run Toggl sync for last 7 days
    print_info "Syncing Toggl time entries..."
    
    sync_output=$(bin/toggl sync 7 2>&1)
    if [[ $? -eq 0 ]]; then
        # Extract entry count from output
        if echo "$sync_output" | grep -q "Synced [0-9]* time entries"; then
            entry_count=$(echo "$sync_output" | grep -oE "Synced [0-9]+ time entries" | grep -oE "[0-9]+" | head -1)
            print_info "Synced $entry_count time entries"
        fi
        
        # Show today's total if available
        today_total=$(sqlite3 .data/today.db "SELECT printf('%.2f', SUM(duration) / 3600.0) FROM toggl_time_entries WHERE DATE(start) = DATE('now', 'localtime') AND stop IS NOT NULL" 2>/dev/null || echo "0")
        if [[ "$today_total" != "0" ]] && [[ -n "$today_total" ]]; then
            print_info "Today's tracked time: ${today_total} hours"
        fi
        
        return 0
    else
        print_error "Toggl sync failed"
        return 1
    fi
}

# Sync contacts
sync_contacts() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if contacts binary exists
    if [[ ! -f bin/contacts ]]; then
        print_error "Contacts CLI not available"
        return 1
    fi
    
    # Check if iCloud credentials are configured
    if ! grep -q "ICLOUD_USERNAME=" .env 2>/dev/null || ! grep -q "ICLOUD_APP_PASSWORD=" .env 2>/dev/null; then
        print_info "iCloud contacts not configured (need ICLOUD_USERNAME and ICLOUD_APP_PASSWORD)"
        return 1
    fi
    
    # Run contacts sync
    print_info "Syncing iCloud contacts..."
    
    sync_output=$(bin/contacts sync 2>&1)
    if [[ $? -eq 0 ]]; then
        # Extract contact count from output
        # Check for synced to database message
        if echo "$sync_output" | grep -q "âœ… Synced [0-9]* contacts to database"; then
            contact_count=$(echo "$sync_output" | grep -oE "[0-9]+ contacts to database" | grep -oE "[0-9]+" | head -1)
            print_info "Synced $contact_count contacts to database"
        # Check for database cache message (new SQLite version)
        elif echo "$sync_output" | grep -q "Loaded [0-9]* contacts from database cache"; then
            contact_count=$(echo "$sync_output" | grep -oE "[0-9]+ contacts from database cache" | grep -oE "[0-9]+" | head -1)
            print_info "Loaded $contact_count contacts from cache"
        # Fallback for old cache message
        elif echo "$sync_output" | grep -q "Loaded [0-9]* contacts from cache"; then
            contact_count=$(echo "$sync_output" | grep -oE "[0-9]+ contacts from cache" | grep -oE "[0-9]+" | head -1)
            print_info "Loaded $contact_count contacts from cache"
        else
            print_info "No contacts found"
        fi
        
        # Show stats (with timeout to prevent hanging)
        stats_output=$(timeout 2 bin/contacts stats 2>&1 | grep -E "Total|With email" | head -2) || true
        if [[ -n "$stats_output" ]]; then
            echo "$stats_output" | while IFS= read -r line; do
                print_info "  $line"
            done
        fi
        
        return 0
    else
        print_error "Contacts sync failed"
        if echo "$sync_output" | grep -q "Authentication failed"; then
            print_info "Check your iCloud app-specific password"
        else
            print_info "Error: $sync_output"
        fi
        return 1
    fi
}

# Sync contact tracking between database and vault/logs/every_six_weeks.md
sync_contact_tracking() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if contacts database exists
    if [[ ! -f .data/today.db ]]; then
        print_info "Contacts database not found"
        return 1
    fi
    
    # Run the contacts sync-tracking command
    print_info "Syncing contact tracking file with database..."
    
    sync_output=$(bin/contacts sync-tracking 2>&1)
    if [[ $? -eq 0 ]]; then
        # Extract stats from output
        if echo "$sync_output" | grep -q "Updated [0-9]* contact"; then
            update_count=$(echo "$sync_output" | grep -oE "Updated [0-9]+ contact" | grep -oE "[0-9]+" | head -1)
            print_info "Updated $update_count contact(s) from file edits"
        fi
        
        if echo "$sync_output" | grep -q "Tracking [0-9]* contacts"; then
            total_count=$(echo "$sync_output" | grep -oE "Tracking [0-9]+ contacts" | grep -oE "[0-9]+" | head -1)
            print_info "Tracking $total_count contacts total"
        fi
        
        # Show overdue contacts if any
        if echo "$sync_output" | grep -q "overdue"; then
            overdue_count=$(echo "$sync_output" | grep -oE "[0-9]+ contact.*overdue" | grep -oE "[0-9]+" | head -1)
            if [[ "$overdue_count" -gt 0 ]]; then
                print_info "âš ï¸  $overdue_count contact(s) overdue for 6+ weeks"
                # Show first few overdue contacts
                echo "$sync_output" | grep "â€¢" | head -3 | while IFS= read -r line; do
                    print_info "  $line"
                done
            fi
        fi
        
        return 0
    else
        print_error "Contact tracking sync failed"
        return 1
    fi
}

# Sync Day One diary entries to database
sync_diary() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if diary-sync script exists
    if [[ ! -f bin/diary-sync.cjs ]]; then
        print_error "Diary sync script not available"
        return 1
    fi
    
    # Check if Journal.json exists
    if [[ ! -f vault/logs/Journal.json ]]; then
        print_info "No Day One journal export found"
        return 1
    fi
    
    # Run diary sync
    print_info "Syncing Day One diary entries..."
    
    sync_output=$(node bin/diary-sync.cjs 2>&1)
    if [[ $? -eq 0 ]]; then
        # Extract stats from output
        if echo "$sync_output" | grep -q "Successfully synced"; then
            entry_count=$(echo "$sync_output" | grep -oE "synced [0-9]+ diary" | grep -oE "[0-9]+" | head -1)
            print_info "Synced $entry_count diary entries"
        elif echo "$sync_output" | grep -q "already up to date"; then
            print_info "Diary already up to date"
        fi
        
        # Show stats
        if echo "$sync_output" | grep -q "Total entries:"; then
            total=$(echo "$sync_output" | grep "Total entries:" | grep -oE "[0-9]+" | head -1)
            recent=$(echo "$sync_output" | grep "Recent entries:" | grep -oE "[0-9]+ \(week\)" | grep -oE "[0-9]+" | head -1)
            if [[ -n "$total" ]]; then
                print_info "Total diary entries: $total (recent: $recent this week)"
            fi
        fi
        
        return 0
    else
        print_error "Diary sync failed"
        if echo "$sync_output" | grep -q "not found"; then
            print_info "Export your Day One journal to vault/logs/Journal.json"
        fi
        return 1
    fi
}

# Sync OlderGay.Men monitoring data
sync_ogm_monitoring() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if ogm-sync script exists
    if [[ ! -f bin/ogm-sync ]]; then
        print_error "OGM sync script not available"
        return 1
    fi
    
    # Run OGM sync
    print_info "Syncing OlderGay.Men monitoring data..."
    
    sync_output=$(env bin/ogm-sync 2>&1)
    if [[ $? -eq 0 ]]; then
        # Extract stats from output
        if echo "$sync_output" | grep -q "Open Issues"; then
            # Parse the summary table
            open_issues=$(echo "$sync_output" | grep -A1 "Open Issues" | tail -1 | awk '{print $1}')
            active_errors=$(echo "$sync_output" | grep -A1 "Open Issues" | tail -1 | awk '{print $2}')
            
            if [[ -n "$open_issues" ]]; then
                print_info "GitHub open issues: $open_issues"
            fi
            if [[ -n "$active_errors" ]]; then
                print_info "HoneyBadger active errors: $active_errors"
            fi
        fi
        
        # Check if performance metrics were synced
        if echo "$sync_output" | grep -q "Scout performance metrics"; then
            print_info "Performance metrics updated"
        fi
        
        return 0
    else
        print_error "OGM monitoring sync failed"
        print_info "Check API tokens in .env file"
        return 1
    fi
}

# Process inbox notes (from Drafts uploads)
process_inbox() {
    cd "$(dirname "$0")/.." || return 1
    
    # Create inbox directory if it doesn't exist
    mkdir -p vault/notes/inbox 2>/dev/null
    
    # Check if there are any files in the inbox
    INBOX_FILES=$(find vault/notes/inbox -type f -name "*.md" 2>/dev/null)
    if [[ -z "$INBOX_FILES" ]]; then
        print_info "No files in inbox to process"
        return 0
    fi
    
    # Count files to process
    FILE_COUNT=$(echo "$INBOX_FILES" | wc -l)
    print_info "Processing $FILE_COUNT file(s) from inbox..."
    
    # Process each file
    while IFS= read -r file; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        BASENAME=$(basename "$file")
        CONTENT=$(cat "$file" 2>/dev/null || echo "")
        FIRST_LINE=$(echo "$CONTENT" | head -1)
        TITLE=$(echo "$FIRST_LINE" | sed 's/^#\s*//' | sed 's/^-\s*\[\s*\]\s*//')
        
        # Determine destination based on content
        DEST_DIR=""
        DEST_FILE=""
        
        # Special case: Streaks file
        if [[ "$TITLE" == "Streaks" ]]; then
            # Convert Streaks items to TaskPaper format with task-id: none
            CONVERTED_CONTENT=$(echo "$CONTENT" | awk '
                NR==1 {print; next}  # Keep header
                /^[^-]/ && /[a-zA-Z]/ {print "- [ ] " $0 " <!-- task-id: none -->"; next}  # Add checkbox and task-id: none
                /^- \[[ x]\]/ && !/task-id:/ {print $0 " <!-- task-id: none -->"; next}  # Add task-id: none if missing
                {print}  # Keep other lines as-is
            ')
            DEST_DIR="vault/tasks"
            DEST_FILE="streaks-today.md"
            echo "$CONVERTED_CONTENT" > "$file"  # Update content for move
            print_info "  â€¢ Streaks file â†’ tasks/streaks-today.md"
        
        # Special case: Gratitude note - append to gratitude log
        elif [[ "$TITLE" == "Gratitude" ]]; then
            # Extract the gratitude line (everything after "I'm grateful for...")
            # First find the line, then get everything after it that's not empty
            GRATITUDE_LINE=$(echo "$CONTENT" | awk '/I.m grateful for/ {found=1; next} found && NF {print; exit}')
            
            if [[ -n "$GRATITUDE_LINE" ]]; then
                # Get today's date in YYYY-MM-DD format
                TODAY_DATE=$(date +%Y-%m-%d)
                GRATITUDE_FILE="vault/logs/gratitude.md"
                
                # Create file if it doesn't exist
                if [[ ! -f "$GRATITUDE_FILE" ]]; then
                    echo "# Gratitude Log" > "$GRATITUDE_FILE"
                    echo "" >> "$GRATITUDE_FILE"
                    echo "<!-- Entries are automatically added from inbox notes -->" >> "$GRATITUDE_FILE"
                    echo "<!-- Format: YYYY-MM-DD entry text -->" >> "$GRATITUDE_FILE"
                    echo "" >> "$GRATITUDE_FILE"
                fi
                
                # Create temp file for processing
                TEMP_FILE=$(mktemp)
                ENTRIES_FILE=$(mktemp)
                
                # Add new entry to a temp entries file
                echo "$TODAY_DATE $GRATITUDE_LINE" > "$ENTRIES_FILE"
                
                # Get existing entries (skip header and blank lines)
                tail -n +5 "$GRATITUDE_FILE" 2>/dev/null | grep -v "^$" >> "$ENTRIES_FILE" 2>/dev/null || true
                
                # Sort entries by date (reverse) and deduplicate
                # The sort -r gives us newest first, and sort -u removes duplicates
                SORTED_ENTRIES=$(sort -r -u "$ENTRIES_FILE")
                
                # Build the final file
                head -4 "$GRATITUDE_FILE" > "$TEMP_FILE"  # Copy header
                echo "" >> "$TEMP_FILE"  # Single blank line after header
                echo "$SORTED_ENTRIES" >> "$TEMP_FILE"  # Add all entries without blank lines between them
                
                # Replace original file and clean up
                mv "$TEMP_FILE" "$GRATITUDE_FILE"
                rm -f "$ENTRIES_FILE"
                
                print_info "  â€¢ Gratitude note â†’ Added to logs/gratitude.md"
                print_info "    $TODAY_DATE: $GRATITUDE_LINE"
                
                # Delete the gratitude note from inbox only after successful processing
                rm "$file"
                continue  # Skip the normal file move logic
            else
                print_error "  â€¢ Gratitude note has no content after 'I'm grateful for...'"
                print_info "    Note kept in inbox - please add content after 'I'm grateful for...'"
                # Don't delete the file - user can fix the format
                continue  # Skip the normal file move logic but keep the file
            fi
        
        # Special case: Progress note - append to today's review
        elif [[ "$TITLE" == "Progress" ]]; then
            # Get date components for new naming scheme
            YEAR=$(date +%Y)
            MONTH=$(date +%m)
            DAY=$(date +%d)
            WEEK=$(date +%V)  # ISO week number
            # Determine quarter from month
            if [ $MONTH -le 3 ]; then Q="Q1"
            elif [ $MONTH -le 6 ]; then Q="Q2"
            elif [ $MONTH -le 9 ]; then Q="Q3"
            else Q="Q4"
            fi
            REVIEW_FILE="vault/plans/${YEAR}_${Q}_${MONTH}_W${WEEK}_${DAY}.md"
            TIMESTAMP=$(date +"%H:%M")
            
            # Extract content (everything after first line)
            PROGRESS_CONTENT=$(echo "$CONTENT" | tail -n +2 | sed '/^$/d')
            
            if [[ -f "$REVIEW_FILE" ]]; then
                # Append to review file
                echo "" >> "$REVIEW_FILE"
                echo "### Progress Update ($TIMESTAMP)" >> "$REVIEW_FILE"
                echo "$PROGRESS_CONTENT" >> "$REVIEW_FILE"
                print_info "  â€¢ Progress note â†’ Added to today's review"
                print_info "    $TIMESTAMP: $(echo "$PROGRESS_CONTENT" | head -1)"
                
                # Delete the progress note from inbox only after successful append
                rm "$file"
                continue  # Skip the normal file move logic
            else
                print_error "  â€¢ No review file for today. Run 'bin/today' first"
                print_info "    Progress note kept in inbox for next sync"
                # Don't delete the file - leave it in inbox for next sync attempt
                continue  # Skip the normal file move logic but keep the file
            fi
        
        # Special case: Concerns file
        elif [[ "$TITLE" == "Concerns" ]] || [[ "$BASENAME" == *"concerns"* ]]; then
            # Remove title line and subsequent blank lines
            CLEANED_CONTENT=$(echo "$CONTENT" | awk '
                NR==1 && /^#.*[Cc]oncerns/ {next}  # Skip title line if it contains "concerns"
                /^$/ && !started {next}            # Skip leading blank lines
                {started=1; print}                 # Print everything else
            ')
            echo "$CLEANED_CONTENT" > "$file"  # Update content for move
            DEST_DIR="vault/notes/concerns"
            DEST_FILE="$BASENAME"
            print_info "  â€¢ Concerns file â†’ concerns/$BASENAME"
        
        # Task files (contains checkboxes)
        elif echo "$CONTENT" | grep -qE "^-\s*\[[ x]\]"; then
            # Check if this file contains ONLY tasks (no other content)
            TASK_ONLY=true
            while IFS= read -r line; do
                # Skip empty lines
                if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*$ ]]; then
                    continue
                fi
                # Check if line is a task
                if ! echo "$line" | grep -qE "^-\s*\[[ x]\]"; then
                    TASK_ONLY=false
                    break
                fi
            done <<< "$CONTENT"
            
            if [[ "$TASK_ONLY" == "true" ]]; then
                # Append to consolidated tasks.md file
                TASKS_FILE="vault/tasks/tasks.md"
                mkdir -p vault/tasks 2>/dev/null
                
                # Insert tasks before Archive section (or at end if no Archive)
                if [[ -f "$TASKS_FILE" ]]; then
                    if grep -q "^# Archive" "$TASKS_FILE"; then
                        # File has Archive section - insert before it
                        TEMP_FILE=$(mktemp)
                        
                        # Get everything before Archive
                        awk '/^# Archive/ {exit} {print}' "$TASKS_FILE" > "$TEMP_FILE"
                        
                        # Remove trailing blank lines from active section
                        sed -i '' -e :a -e '/^\s*$/d;N;ba' "$TEMP_FILE" 2>/dev/null || \
                        sed -i -e :a -e '/^\s*$/d;N;ba' "$TEMP_FILE" 2>/dev/null || true
                        
                        # Add the new tasks
                        echo "" >> "$TEMP_FILE"
                        echo "$CONTENT" >> "$TEMP_FILE"
                        
                        # Add Archive section back
                        echo "" >> "$TEMP_FILE"
                        awk '/^# Archive/,EOF {print}' "$TASKS_FILE" >> "$TEMP_FILE"
                        
                        # Replace original file
                        mv "$TEMP_FILE" "$TASKS_FILE"
                    else
                        # No Archive section - just append at end
                        if [[ -n "$(tail -c 1 "$TASKS_FILE")" ]]; then
                            echo "" >> "$TASKS_FILE"
                        fi
                        echo "$CONTENT" >> "$TASKS_FILE"
                    fi
                else
                    # File doesn't exist - create it
                    echo "$CONTENT" > "$TASKS_FILE"
                fi
                
                print_info "  â€¢ Task-only file â†’ Added to tasks/tasks.md"
                print_info "    Added $(echo "$CONTENT" | grep -c "^-\s*\[[ x]\]") task(s)"
                
                # Delete the original file from inbox
                rm "$file"
                continue  # Skip the normal file move logic
            else
                # Mixed content - keep as separate file
                DEST_DIR="vault/tasks"
                DEST_FILE="$BASENAME"
                print_info "  â€¢ Mixed task file â†’ tasks/$BASENAME"
            fi
        
        # Daily notes (default)
        else
            DEST_DIR="vault/notes/daily"
            DEST_FILE="$BASENAME"
            print_info "  â€¢ Daily note â†’ daily/$BASENAME"
        fi
        
        # Create destination directory if needed
        mkdir -p "$DEST_DIR" 2>/dev/null
        
        # Move the file
        DEST_PATH="$DEST_DIR/$DEST_FILE"
        if [[ -f "$DEST_PATH" ]] && [[ "$DEST_FILE" != "streaks-today.md" ]]; then
            # File exists and it's not streaks (which we always overwrite)
            print_info "    File exists, skipping: $DEST_FILE"
            rm "$file"  # Remove from inbox since it's a duplicate
        else
            mv "$file" "$DEST_PATH" 2>/dev/null && {
                print_info "    âœ“ Filed to $DEST_PATH"
            } || {
                print_error "    Failed to move $BASENAME"
            }
        fi
    done <<< "$INBOX_FILES"
    
    # Commit any moved files
    if [[ -n $(git status --porcelain vault/ 2>/dev/null) ]]; then
        print_info "Committing filed notes..."
        git add vault/ 2>/dev/null
        git commit -m "Process inbox: file notes to appropriate directories" --quiet 2>/dev/null
        git push origin main --quiet 2>/dev/null
    fi
    
    return 0
}

# Sync tasks to their project markdown files
sync_project_tasks() {
    cd "$(dirname "$0")/.." || return 1
    
    print_info "Syncing tasks to project markdown files..."
    
    # Get all active tasks with projects
    TASKS_WITH_PROJECTS=$(sqlite3 .data/today.db "
        SELECT 
            t.id,
            t.title,
            t.status,
            t.description,
            p.name as project_name,
            p.file_path
        FROM tasks t
        JOIN projects p ON t.project_id = p.id
        WHERE t.status <> 'done'
        ORDER BY p.file_path, t.title;
    " 2>/dev/null || echo "")
    
    if [[ -z "$TASKS_WITH_PROJECTS" ]]; then
        print_info "No tasks with projects to sync"
        return 0
    fi
    
    # Count unique projects
    PROJECT_COUNT=$(echo "$TASKS_WITH_PROJECTS" | cut -d'|' -f6 | sort -u | wc -l | tr -d ' ')
    TASK_COUNT=$(echo "$TASKS_WITH_PROJECTS" | wc -l | tr -d ' ')
    print_info "Found $TASK_COUNT task(s) across $PROJECT_COUNT project(s)"
    
    # Process each project's tasks
    CURRENT_PROJECT=""
    PROJECT_TASKS=""
    SYNCED_COUNT=0
    
    while IFS='|' read -r task_id title status description project_name file_path; do
        # Skip if no file path
        if [[ -z "$file_path" ]]; then
            continue
        fi
        
        # Ensure file path starts with vault/ if it doesn't already
        if [[ ! "$file_path" =~ ^vault/ ]] && [[ ! "$file_path" =~ ^/ ]]; then
            file_path="vault/$file_path"
        fi
        
        # When we hit a new project, process the previous one
        if [[ "$file_path" != "$CURRENT_PROJECT" ]] && [[ -n "$CURRENT_PROJECT" ]]; then
            # Process the accumulated tasks for the previous project
            _sync_tasks_to_project_file "$CURRENT_PROJECT" "$PROJECT_TASKS"
            PROJECT_TASKS=""
        fi
        
        CURRENT_PROJECT="$file_path"
        
        # Format task with checkbox and task-id
        TASK_LINE="- [ ] $title <!-- task-id: $task_id -->"
        if [[ -n "$PROJECT_TASKS" ]]; then
            PROJECT_TASKS="$PROJECT_TASKS"$'\n'"$TASK_LINE"
        else
            PROJECT_TASKS="$TASK_LINE"
        fi
    done <<< "$TASKS_WITH_PROJECTS"
    
    # Process the last project
    if [[ -n "$CURRENT_PROJECT" ]] && [[ -n "$PROJECT_TASKS" ]]; then
        _sync_tasks_to_project_file "$CURRENT_PROJECT" "$PROJECT_TASKS"
    fi
    
    print_info "Synced tasks to $PROJECT_COUNT project file(s)"
    return 0
}

# Helper function to sync tasks to a single project file
_sync_tasks_to_project_file() {
    local project_file="$1"
    local tasks_to_add="$2"
    
    # Check if project file exists
    if [[ ! -f "$project_file" ]]; then
        print_info "  Creating project file: $project_file"
        mkdir -p "$(dirname "$project_file")" 2>/dev/null
        
        # Extract project name from file path
        local project_name=$(basename "$project_file" .md | tr '-' ' ' | sed 's/\b\(.\)/\u\1/g')
        
        # Create file with title and tasks
        {
            echo "# $project_name"
            echo ""
            echo "## Tasks"
            echo ""
            echo "$tasks_to_add"
        } > "$project_file"
        
        SYNCED_COUNT=$((SYNCED_COUNT + 1))
        return 0
    fi
    
    # File exists - check which tasks are already present
    local tasks_added=0
    local temp_file=$(mktemp)
    
    # Read the file and find where to insert tasks
    local in_header=true
    local found_tasks_section=false
    local tasks_to_insert=""
    
    # Check each task to see if it's already in the file
    while IFS= read -r task_line; do
        # Extract task ID from the line
        local task_id=$(echo "$task_line" | grep -oE 'task-id: [a-f0-9]{32}' | cut -d' ' -f2)
        if [[ -n "$task_id" ]]; then
            # Check if this task ID already exists in the file
            if ! grep -q "task-id: $task_id" "$project_file" 2>/dev/null; then
                if [[ -n "$tasks_to_insert" ]]; then
                    tasks_to_insert="$tasks_to_insert"$'\n'"$task_line"
                else
                    tasks_to_insert="$task_line"
                fi
                tasks_added=$((tasks_added + 1))
            fi
        fi
    done <<< "$tasks_to_add"
    
    # If no new tasks to add, skip
    if [[ $tasks_added -eq 0 ]]; then
        return 0
    fi
    
    print_info "  Adding $tasks_added task(s) to $(basename "$project_file")"
    
    # Simply append the new tasks to the end of the file 
    # Since we already verified these task IDs don't exist in the file,
    # we can safely append them without worrying about duplication
    
    # Append tasks to the end of the file
    {
        cat "$project_file"
        echo ""
        echo "## Tasks"
        echo ""
        echo "$tasks_to_insert"
        echo ""
    } > "$temp_file"
    
    # Replace the original file
    mv "$temp_file" "$project_file"
    SYNCED_COUNT=$((SYNCED_COUNT + 1))
}

# Archive completed tasks in tasks.md
archive_completed_tasks() {
    cd "$(dirname "$0")/.." || return 1
    
    TASKS_FILE="vault/tasks/tasks.md"
    
    # Check if tasks.md exists
    if [[ ! -f "$TASKS_FILE" ]]; then
        print_info "No tasks.md file found"
        return 0
    fi
    
    # Get today's date
    TODAY=$(date +%Y-%m-%d)
    
    # Create temporary files
    ACTIVE_TASKS=$(mktemp)
    COMPLETED_NEW=$(mktemp)
    ARCHIVE_CONTENT=$(mktemp)
    
    # Extract sections from the file
    # 1. Active section - preserve ALL structure (headers, tasks, blank lines) except completed tasks
    # This keeps project headers (## Project Name) intact
    awk '
        /^# Archive/ {exit}
        /^- \[x\]/ {next}  # Skip completed tasks
        {print}            # Keep everything else (headers, uncompleted tasks, blank lines)
    ' "$TASKS_FILE" | sed '/^$/N;/^\n$/d' > "$ACTIVE_TASKS" || true
    
    # 2. NEW completed tasks from active section (to be archived)
    awk '/^# Archive/ {exit} /^- \[x\]/ {print}' "$TASKS_FILE" > "$COMPLETED_NEW" || true
    
    # 3. Existing archive content (deduplicated)
    if grep -q "^# Archive" "$TASKS_FILE"; then
        awk '
            /^# Archive/ {in_archive=1; next}
            in_archive && /^## [0-9]{4}-[0-9]{2}-[0-9]{2}/ {
                current_date = substr($0, 4)
                print
                next
            }
            in_archive && /^- \[x\]/ {
                # Store task to check for duplicates
                task = $0
                if (!(task in seen)) {
                    seen[task] = 1
                    print
                }
                next
            }
            in_archive && /^- \[ \]/ {
                # Move incomplete tasks back to active
                next
            }
            in_archive {print}
        ' "$TASKS_FILE" > "$ARCHIVE_CONTENT" || true
    fi
    
    # Check if there are any new completed tasks to archive
    NEW_COUNT=$(wc -l < "$COMPLETED_NEW" | tr -d ' ')
    if [[ "$NEW_COUNT" -eq 0 ]]; then
        # No new completed tasks, but still rebuild to clean duplicates
        {
            # Active tasks
            if [[ -s "$ACTIVE_TASKS" ]]; then
                cat "$ACTIVE_TASKS"
            fi
            
            # Archive section
            if [[ -s "$ARCHIVE_CONTENT" ]] || grep -q "^# Archive" "$TASKS_FILE"; then
                echo ""
                echo "# Archive"
                echo ""
                if [[ -s "$ARCHIVE_CONTENT" ]]; then
                    cat "$ARCHIVE_CONTENT"
                fi
            fi
        } > "${TASKS_FILE}.tmp"
        
        mv "${TASKS_FILE}.tmp" "$TASKS_FILE"
        rm -f "$ACTIVE_TASKS" "$COMPLETED_NEW" "$ARCHIVE_CONTENT"
        
        print_info "No new completed tasks to archive, cleaned duplicates"
        return 0
    fi
    
    # Deduplicate new completed tasks
    sort -u "$COMPLETED_NEW" > "${COMPLETED_NEW}.dedup"
    mv "${COMPLETED_NEW}.dedup" "$COMPLETED_NEW"
    
    # Build the new file
    {
        # Active tasks first
        if [[ -s "$ACTIVE_TASKS" ]]; then
            cat "$ACTIVE_TASKS"
        fi
        
        # Archive section
        echo ""
        echo "# Archive"
        echo ""
        
        # Add today's section with new completed tasks
        echo "## $TODAY"
        cat "$COMPLETED_NEW"
        
        # Add existing archive content (if any)
        if [[ -s "$ARCHIVE_CONTENT" ]]; then
            echo ""
            cat "$ARCHIVE_CONTENT"
        fi
    } > "${TASKS_FILE}.tmp"
    
    # Replace the original file
    mv "${TASKS_FILE}.tmp" "$TASKS_FILE"
    
    # Clean up temp files
    rm -f "$ACTIVE_TASKS" "$COMPLETED_NEW" "$ARCHIVE_CONTENT"
    
    # Count unique completed tasks archived
    UNIQUE_COUNT=$(wc -l < "$COMPLETED_NEW" 2>/dev/null | tr -d ' ' || echo 0)
    print_info "Archived $UNIQUE_COUNT completed task(s) to $TODAY section"
    
    return 0
}


# Update cache statistics
update_cache_stats() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if cache database exists
    CACHE_DB=".data/today.db"
    if [[ ! -f "$CACHE_DB" ]]; then
        print_info "Cache database not found, initializing..."
        mkdir -p .data
        # Create an empty database with the required tables
        sqlite3 "$CACHE_DB" <<EOF
CREATE TABLE IF NOT EXISTS emails (
    id TEXT PRIMARY KEY,
    subject TEXT,
    from_address TEXT,
    date TEXT,
    has_been_replied_to INTEGER DEFAULT 0,
    text_content TEXT
);
CREATE TABLE IF NOT EXISTS task_cache (
    id TEXT PRIMARY KEY,
    title TEXT,
    due_date TEXT,
    stage TEXT,
    tags TEXT,
    description TEXT,
    last_edited_time TEXT
);
CREATE TABLE IF NOT EXISTS database_cache (
    id TEXT PRIMARY KEY,
    data TEXT
);
CREATE TABLE IF NOT EXISTS cache_metadata (
    key TEXT PRIMARY KEY,
    value TEXT
);
CREATE TABLE IF NOT EXISTS project_pillar_mapping (
    project_id TEXT,
    pillar_id TEXT
);
EOF
        print_info "Cache database initialized"
    fi
    
    # Get cache statistics
    cache_output=$(node -e "
        // Use environment variables already loaded by parent script
        import('./src/sqlite-cache.js').then(({default: DatabaseCache}) => {
            const cache = new DatabaseCache();
            const stats = cache.getStatistics();
            if (stats && stats.totalItems !== undefined) {
                console.log('Items: ' + stats.totalItems);
                console.log('Databases: ' + stats.totalDatabases);
                const sizeInMB = stats.cacheSize ? (stats.cacheSize / 1024 / 1024).toFixed(2) : '0.00';
                console.log('Size: ' + sizeInMB + ' MB');
            } else {
                console.log('Cache initialized');
            }
            cache.close();
        }).catch((err) => {
            console.error('Cache error: ' + err.message);
            process.exit(1);
        });
    " 2>&1)
    
    if [[ $? -eq 0 ]]; then
        if [[ -n "$cache_output" ]]; then
            while IFS= read -r line; do
                print_info "$line"
            done <<< "$cache_output"
        fi
        return 0
    else
        # Silently succeed - cache errors aren't critical
        return 0
    fi
}

# Parse command line arguments
case "${1:-}" in
    --cleanup-conflicts)
        # Only cleanup sync-conflict files
        print_header "ðŸ—‘ï¸ Cleanup Sync Conflicts"
        if cleanup_sync_conflicts; then
            print_status "Sync conflicts cleaned successfully!"
        else
            print_error "Sync conflict cleanup failed"
        fi
        exit 0
        ;;
    --process-inbox-only)
        # Only process inbox files, no other sync operations
        print_header "ðŸ“¥ Process Inbox Only"
        if process_inbox; then
            print_status "Inbox processed successfully!"
        else
            print_error "Inbox processing failed"
        fi
        exit 0
        ;;
    --vault)
        # Sync only GitHub vault and process inbox
        print_header "ðŸ“ GitHub Vault"
        if sync_github_vault; then
            print_status "Vault synced successfully!"
        else
            print_error "Notes sync failed"
        fi
        
        # Clean markdown files after syncing (unless SKIP_MARKDOWN_CLEAN is set)
        if [[ -z "$SKIP_MARKDOWN_CLEAN" ]]; then
            print_header "ðŸ§¹ Clean Markdown"
            if clean_markdown_files; then
                print_status "Markdown cleaned successfully!"
            else
                print_error "Markdown cleaning failed"
            fi
        fi
        
        # Also process inbox when syncing vault
        print_header "ðŸ“¥ Process Inbox"
        if process_inbox; then
            print_status "Inbox processed successfully!"
        else
            print_error "Inbox processing failed"
        fi
        
        # Archive completed tasks
        print_header "âœ… Archive Tasks"
        if archive_completed_tasks; then
            print_status "Tasks archived successfully!"
        fi
        ;;
    --quick)
        # Quick sync - only critical sources
        print_info "Quick sync mode - GitHub vault, tasks, inbox, and consistency check"
        # NOTE: sync_github_vault removed - using Syncthing
        cleanup_sync_conflicts
        clean_markdown_files
        clean_database_task_titles
        process_inbox
        SKIP_CLASSIFICATION=1 sync_tasks
        sync_project_tasks
        check_task_consistency
        
        # NOTE: Final vault push removed - Syncthing handles this
        # sync_github_vault
        ;;
    --force)
        # Force full sync - bypasses smart caching
        print_info "Force sync mode - refreshing all data"
        FORCE_SYNC=true
        main
        ;;
    --quick-email)
        # Just sync last day of email quickly
        print_header "ðŸ“§ Quick Email Sync"
        EMAIL_BIN=""
        if [[ -f bin/email ]]; then
            EMAIL_BIN="bin/email"
        elif [[ -f bin/email-cli ]]; then
            EMAIL_BIN="bin/email-cli"
        fi
        if [[ -n "$EMAIL_BIN" ]]; then
            print_info "Downloading emails from last 24 hours..."
            timeout 30 $EMAIL_BIN download --days 1 | grep -E "Downloaded|Total|âœ…" || true
            print_status "Quick email sync complete"
        else
            print_error "Email CLI not found"
        fi
        ;;
    --help|-h)
        echo "Usage: sync [OPTIONS]"
        echo ""
        echo "Intelligently synchronize all data sources"
        echo ""
        echo "Runs all operations synchronously for reliability:"
        echo "  â€¢ Turso bidirectional sync (newest data wins)"
        echo "  â€¢ GitHub vault, task sync, markdown cleanup"
        echo "  â€¢ Task classification, email, calendar, contacts"
        echo "  â€¢ Uses efficient timeouts and smart caching"
        echo ""
        echo "Options:"
        echo "  --cleanup-conflicts  Remove duplicate sync-conflict files"
        echo "  --vault             (Deprecated - vault now uses Syncthing)"
        echo "  --quick             Quick sync (tasks and inbox only)"
        echo "  --force             Force full refresh (bypass cache)"
        echo "  --quick-email       Download last 24 hours of email"
        echo "  --help              Show this help message"
        echo ""
        echo "Data sources synced:"
        echo "  â€¢ Vault files (via Syncthing - real-time sync)"
        echo "  â€¢ Markdown formatting cleanup (via markdownlint)"
        echo "  â€¢ Local task manager"
        echo "  â€¢ Email inbox (cached for 4 hours)"
        echo "  â€¢ Calendar events"
        echo "  â€¢ iCloud contacts (cached for 4 hours)"
        echo "  â€¢ Inbox processing (files from Drafts)"
        echo "  â€¢ Local cache"
        ;;
    *)
        main
        ;;
esac