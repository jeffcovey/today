#!/bin/bash
# Unified sync script - Pull all external data sources
# This script synchronizes all data sources to prepare for AI review

set -e

# Load common dotenvx handler
source "$(dirname "$0")/lib/dotenvx-loader.sh"
auto_dotenvx "$@"

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}âœ“${NC} $1"
}

print_error() {
    echo -e "${RED}âœ—${NC} $1"
}

print_info() {
    echo -e "${BLUE}â„¹${NC} $1"
}

print_header() {
    echo ""
    echo -e "${YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${YELLOW}$1${NC}"
    echo -e "${YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
}

# Track what was synced
SYNC_SUMMARY=""

# Helper function to run a sync step with timing
run_sync_step() {
    local header="$1"
    local func="$2"
    local success_msg="$3"
    local summary_name="$4"
    
    print_header "$header"
    START_TIME=$(date +%s)
    if $func; then
        print_status "$success_msg"
        SYNC_SUMMARY="${SYNC_SUMMARY}âœ“ $summary_name\n"
        local result=0
    else
        print_error "$summary_name sync failed"
        SYNC_SUMMARY="${SYNC_SUMMARY}âœ— $summary_name\n"
        local result=1
    fi
    END_TIME=$(date +%s)
    print_info "â±ï¸  Took $((END_TIME - START_TIME)) seconds"
    return $result
}

# Main sync function
main() {
    echo ""
    echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${BLUE}â•‘       TODAY - Full Data Sync          â•‘${NC}"
    echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
    echo "Synchronizing all data sources..."
    echo "Started at: $(date '+%Y-%m-%d %H:%M:%S')"
    
    # Run all sync steps
    run_sync_step "ðŸ“ GitHub Notes" sync_github_notes "GitHub notes synced" "GitHub notes"
    run_sync_step "ðŸ“Š Notion Database" sync_notion "Notion database synced" "Notion database"
    
    # Run Todoist sync
    run_sync_step "âœ… Todoist Tasks" sync_todoist "Todoist tasks synced" "Todoist tasks"
    
    run_sync_step "ðŸ“§ Email Inbox" sync_email "Email sync complete" "Email inbox"
    run_sync_step "ðŸ“… Calendar Events" sync_calendar "Calendar events synced" "Calendar events"
    run_sync_step "ðŸ“¥ Process Inbox" process_inbox "Inbox processed" "Inbox processed"
    run_sync_step "âœ… Archive Tasks" archive_completed_tasks "Tasks archived" "Tasks archived"
    run_sync_step "ðŸ’¾ Cache Update" update_cache_stats "Cache updated" "Cache updated"
    
    # Print summary
    print_header "ðŸ“‹ Sync Summary"
    echo -e "$SYNC_SUMMARY"
    echo "Completed at: $(date '+%Y-%m-%d %H:%M:%S')"
    echo ""
    
    # Check if any sync failed
    if echo -e "$SYNC_SUMMARY" | grep -q "âœ—"; then
        print_error "Some data sources failed to sync"
        echo "Please configure missing services in .env file"
        exit 1
    else
        print_status "All data sources synchronized!"
    fi
    
    echo ""
    echo "Next step: Run 'bin/today' to get AI suggestions on what to do today"
}

# Sync GitHub notes
sync_github_notes() {
    # Change to project directory
    cd "$(dirname "$0")/.." || return 1
    
    # First, check if we can pull cleanly (no uncommitted changes in repo)
    if [[ -n $(git status --porcelain 2>/dev/null) ]]; then
        # Check if there are changes ONLY in notes/ (no other staged or unstaged changes)
        NOTES_CHANGES=$(git status notes/ --porcelain 2>/dev/null)
        OTHER_CHANGES=$(git status --porcelain 2>/dev/null | grep -v "^.. notes/" || true)
        
        if [[ -n "$NOTES_CHANGES" ]] && [[ -z "$OTHER_CHANGES" ]]; then
            # Only notes changes, safe to commit
            print_info "Pushing local note changes..."
            git add notes/ 2>/dev/null
            git commit -m "Sync local notes" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
        elif [[ -n "$NOTES_CHANGES" ]]; then
            # We have both notes changes and other changes
            # First, stash any staged changes outside of notes/
            if [[ -n $(git diff --cached --name-only | grep -v "^notes/" || true) ]]; then
                print_info "Saving staged changes outside notes/..."
                # Create a temporary commit to preserve staged state
                git commit --quiet -m "temp: staged changes" -- ':!notes/' 2>/dev/null || true
                TEMP_COMMIT=true
            fi
            
            # Now commit just the notes
            print_info "Pushing local note changes..."
            git add notes/ 2>/dev/null
            git commit -m "Sync local notes" --quiet 2>/dev/null
            git push origin main --quiet 2>/dev/null
            
            # Restore the temporary commit if we made one
            if [[ "${TEMP_COMMIT:-false}" == "true" ]]; then
                print_info "Restoring staged changes..."
                git reset --soft HEAD~1 2>/dev/null
            fi
        else
            # Changes in other files but not notes - stash them temporarily
            print_info "Stashing local changes outside notes/..."
            git stash push --quiet -m "sync-auto-stash" -- ':!notes/' 2>/dev/null
            STASHED=true
        fi
    fi
    
    # Pull latest changes from remote
    print_info "Pulling latest changes from GitHub..."
    if git pull origin main --quiet 2>/dev/null; then
        PULL_RESULT=$(git log --oneline -1 --format="%s")
        if [[ "$PULL_RESULT" != *"Already up to date"* ]]; then
            print_status "Pulled latest changes from GitHub"
        fi
    else
        # Pull failed - check if it's a merge conflict
        if git status | grep -q "You have unmerged paths"; then
            print_error "Merge conflict detected - manual resolution needed"
            print_info "Run 'git status' to see conflicts"
            return 1
        else
            print_info "No remote changes to pull"
        fi
    fi
    
    # Restore stashed changes if we stashed them
    if [[ "${STASHED:-false}" == "true" ]]; then
        print_info "Restoring stashed changes..."
        git stash pop --quiet 2>/dev/null || true
    fi
    
    # Clean up empty files
    find notes -type f -empty -delete 2>/dev/null
    deleted_count=$(git status --porcelain notes/ | grep -c "^ D " || true)
    if [[ $deleted_count -gt 0 ]]; then
        print_info "Removed $deleted_count empty file(s)"
        git add notes/
        git commit -m "Clean up empty note files" --quiet 2>/dev/null
        git push origin main --quiet 2>/dev/null
    fi
    
    # Show note count
    note_count=$(find notes -type f -name "*.md" 2>/dev/null | wc -l)
    print_info "Total notes: $note_count"
    
    # Count OGM work files
    ogm_count=$(find notes/ogm-work -type f -name "*.md" 2>/dev/null | wc -l || echo 0)
    if [[ $ogm_count -gt 0 ]]; then
        print_info "OlderGay.Men work summaries: $ogm_count"
    fi
    
    # Show recent notes (last 7 days)
    recent_count=$(find notes -type f -name "*.md" -mtime -7 2>/dev/null | wc -l)
    if [[ $recent_count -gt 0 ]]; then
        print_info "Notes from last 7 days: $recent_count"
        # Show up to 5 most recent notes
        print_info "Most recent notes:"
        find notes -type f -name "*.md" -mtime -7 2>/dev/null | head -5 | while read -r note; do
            echo "  â€¢ $(basename "$note")"
        done
    fi
    
    return 0
}

# Sync Notion database
sync_notion() {
    # Use the notion CLI to refresh cache
    cd "$(dirname "$0")/.." || return 1
    
    # Check if NOTION_TOKEN is configured
    if ! grep -q "NOTION_TOKEN=" .env 2>/dev/null || grep -q "NOTION_TOKEN=your_notion" .env 2>/dev/null; then
        print_error "Notion not configured"
        print_info "Add NOTION_TOKEN to .env file"
        return 1
    fi
    
    # Set up cache database
    CACHE_DB=".notion-cache/notion-cache.db"
    
    # Ensure sync_metadata table exists
    if [[ -f "$CACHE_DB" ]]; then
        sqlite3 "$CACHE_DB" "CREATE TABLE IF NOT EXISTS sync_metadata (key TEXT PRIMARY KEY, value TEXT)" 2>/dev/null
        LAST_SYNC=$(sqlite3 "$CACHE_DB" "SELECT value FROM sync_metadata WHERE key='last_full_sync'" 2>/dev/null || echo "")
    else
        print_info "No cache found, initial sync needed..."
        LAST_SYNC=""
    fi
    
    # For now, use the regular fetch-tasks which is fast when cached
    print_info "Syncing Notion tasks..."
    
    # Fetch tasks (will use cache intelligently)
    task_output=$(bin/notion fetch-tasks 2>&1 | tail -5)
    if [[ $? -eq 0 ]]; then
        echo "$task_output" | while IFS= read -r line; do
            print_info "$line"
        done
        
        # Update sync timestamp
        sqlite3 "$CACHE_DB" "INSERT OR REPLACE INTO sync_metadata (key, value) VALUES ('last_full_sync', datetime('now'))" 2>/dev/null
        
        return 0
    else
        print_error "Notion connection failed"
        if echo "$output" | grep -q "NOTION_TOKEN"; then
            print_info "Please configure NOTION_TOKEN in .env file"
        else
            print_info "Error: $output"
        fi
        return 1
    fi
}

# Sync Todoist tasks
sync_todoist() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if Todoist token is configured
    if ! grep -q "TODOIST_TOKEN=" .env 2>/dev/null || grep -q "TODOIST_TOKEN=your_todoist" .env 2>/dev/null; then
        print_info "Todoist not configured, skipping"
        return 0
    fi
    
    # Check for --force flag
    FORCE_MODE=false
    if [[ "${FORCE_SYNC:-}" == "true" ]]; then
        FORCE_MODE=true
    fi
    
    # Run incremental two-way sync directly using Node
    print_info "Syncing with Todoist..."
    
    sync_output=$(node -e "
        import('./src/todoist-sync.js').then(async ({TodoistSync}) => {
            const {NotionAPI} = await import('./src/notion-api.js');
            
            const notionAPI = new NotionAPI(process.env.NOTION_TOKEN);
            const sync = new TodoistSync(process.env.TODOIST_TOKEN, notionAPI);
            
            // Get Action Items database
            const dbs = await notionAPI.getDatabases();
            const actionDB = dbs.find(db => db.title.toLowerCase().includes('action items'));
            if (!actionDB) {
                console.error('Action Items database not found');
                process.exit(1);
            }
            
            // Perform incremental sync (using cached Notion data)
            const result = await sync.performTwoWaySync(actionDB.id, 'Notion Tasks', false);
            
            // Show summary
            const n2t = result.notionToTodoist;
            const t2n = result.todoistToNotion;
            
            if (n2t.created || n2t.updated || t2n.created || t2n.updated) {
                console.log('Changes synced:');
                if (n2t.created + n2t.updated > 0) {
                    console.log('  Notion â†’ Todoist: ' + n2t.created + ' new, ' + n2t.updated + ' updated');
                }
                if (t2n.created + t2n.updated > 0) {
                    console.log('  Todoist â†’ Notion: ' + t2n.created + ' new, ' + t2n.updated + ' updated');
                }
            } else {
                console.log('No changes to sync (7 tasks unchanged)');
            }
        }).catch(err => {
            console.error('Sync failed: ' + err.message);
            process.exit(1);
        });
    " 2>&1)
    
    if [[ $? -eq 0 ]]; then
        echo "$sync_output" | while IFS= read -r line; do
            print_info "$line"
        done
        return 0
    else
        print_error "Todoist sync failed"
        echo "$sync_output" | while IFS= read -r line; do
            print_info "$line"
        done
        return 1
    fi
}

# Sync email
sync_email() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if email-cli/email binary exists
    EMAIL_BIN=""
    if [[ -f bin/email ]]; then
        EMAIL_BIN="bin/email"
    elif [[ -f bin/email-cli ]]; then
        EMAIL_BIN="bin/email-cli"
    else
        print_error "Email CLI not available"
        return 1
    fi
    
    # Check if email credentials are configured
    if ! grep -q "EMAIL_ACCOUNT=" .env 2>/dev/null || ! grep -q "EMAIL_PASSWORD=" .env 2>/dev/null; then
        print_error "Email not configured"
        print_info "Run 'email setup' for configuration instructions"
        return 1
    fi
    
    # Incremental email sync - fetch emails newer than what we have
    if [[ -f .notion-cache/notion-cache.db ]]; then
        # Get the timestamp of the most recent email
        EMAIL_INFO=$(sqlite3 .notion-cache/notion-cache.db "SELECT MAX(date), COUNT(*) FROM emails" 2>/dev/null || echo "||0")
        LAST_EMAIL_DATE=$(echo "$EMAIL_INFO" | cut -d'|' -f1)
        EMAIL_COUNT=$(echo "$EMAIL_INFO" | cut -d'|' -f2)
        
        if [[ -n "$LAST_EMAIL_DATE" ]] && [[ "$EMAIL_COUNT" -gt 0 ]]; then
            # Calculate days since last email to determine sync range
            DAYS_OLD=$(node -e "
                const date = new Date('$LAST_EMAIL_DATE');
                const now = new Date();
                const daysSince = (now - date) / (1000 * 60 * 60 * 24);
                console.log(Math.ceil(daysSince));
            " 2>/dev/null || echo "7")
            
            # Sync enough days to catch all new emails since last sync
            # Add 1 day buffer to ensure we don't miss anything
            DAYS_TO_SYNC=$((DAYS_OLD + 1))
            
            # Cap at 7 days max for performance
            if [[ $DAYS_TO_SYNC -gt 7 ]]; then
                DAYS_TO_SYNC=7
                print_info "Syncing emails from last 7 days (max range)..."
            else
                print_info "Syncing emails from last $DAYS_TO_SYNC days to catch all new mail..."
            fi
        else
            print_info "No emails in database, initial sync of 7 days..."
            DAYS_TO_SYNC=7
        fi
    else
        print_info "No email database found, initial sync of 7 days..."
        DAYS_TO_SYNC=7
    fi
    
    # Use shorter timeout for efficiency
    TIMEOUT=$((10 + $DAYS_TO_SYNC * 5))  # 10s base + 5s per day
    print_info "Fetching new emails (timeout: ${TIMEOUT}s)..."
    
    output=$(timeout $TIMEOUT $EMAIL_BIN download --days $DAYS_TO_SYNC 2>&1) || {
        if [[ $? -eq 124 ]]; then
            print_error "Email download timed out after $TIMEOUT seconds"
            print_info "Try running 'email download --days 1' for a quicker sync"
        else
            print_error "Email download failed"
        fi
        return 1
    }
    
    # Show download results
    if echo "$output" | grep -q "Downloaded"; then
        # Extract number of emails downloaded
        email_count=$(echo "$output" | grep -oE "Downloaded [0-9]+ email" | grep -oE "[0-9]+") || true
        if [[ -n "$email_count" ]]; then
            print_info "Downloaded $email_count new emails"
        fi
    else
        print_info "No new emails to download"
    fi
    
    # Show email stats
    stats_output=$($EMAIL_BIN stats 2>&1 | grep "Total emails:" | cut -d: -f2 | xargs) || true
    if [[ -n "$stats_output" ]]; then
        print_info "Total emails in database: $stats_output"
    fi
    
    return 0
}

# Sync calendar events
sync_calendar() {
    cd "$(dirname "$0")/.." || return 1
    
    # Run calendar sync
    SYNC_OUTPUT=$(bin/calendar sync 2>&1)
    if echo "$SYNC_OUTPUT" | grep -q "âœ… Synced [1-9]"; then
        # Actually synced some events (not 0)
        # Show quick summary of today's events
        TODAY_COUNT=$(bin/calendar today 2>/dev/null | jq '. | length' 2>/dev/null || echo 0)
        if [[ "$TODAY_COUNT" -gt 0 ]]; then
            print_info "$TODAY_COUNT events scheduled for today"
        fi
        return 0
    else
        print_info "No calendars configured (run 'bin/setup --calendar' for instructions)"
        return 1
    fi
}

# Process inbox notes (from Drafts uploads)
process_inbox() {
    cd "$(dirname "$0")/.." || return 1
    
    # Create inbox directory if it doesn't exist
    mkdir -p notes/inbox 2>/dev/null
    
    # Check if there are any files in the inbox
    INBOX_FILES=$(find notes/inbox -type f -name "*.md" 2>/dev/null)
    if [[ -z "$INBOX_FILES" ]]; then
        print_info "No files in inbox to process"
        return 0
    fi
    
    # Count files to process
    FILE_COUNT=$(echo "$INBOX_FILES" | wc -l)
    print_info "Processing $FILE_COUNT file(s) from inbox..."
    
    # Process each file
    while IFS= read -r file; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        BASENAME=$(basename "$file")
        CONTENT=$(cat "$file" 2>/dev/null || echo "")
        FIRST_LINE=$(echo "$CONTENT" | head -1)
        TITLE=$(echo "$FIRST_LINE" | sed 's/^#\s*//' | sed 's/^-\s*\[\s*\]\s*//')
        
        # Determine destination based on content
        DEST_DIR=""
        DEST_FILE=""
        
        # Special case: Streaks file
        if [[ "$TITLE" == "Streaks" ]]; then
            # Convert Streaks items to TaskPaper format if needed
            CONVERTED_CONTENT=$(echo "$CONTENT" | awk '
                NR==1 {print; next}  # Keep header
                /^[^-]/ && /[a-zA-Z]/ {print "- [ ] " $0; next}  # Add checkbox to non-checkbox lines
                {print}  # Keep other lines as-is
            ')
            DEST_DIR="notes/tasks"
            DEST_FILE="streaks-today.md"
            echo "$CONVERTED_CONTENT" > "$file"  # Update content for move
            print_info "  â€¢ Streaks file â†’ tasks/streaks-today.md"
        
        # Special case: Progress note - append to today's review
        elif [[ "$TITLE" == "Progress" ]]; then
            TODAY_DATE=$(date +%Y-%m-%d)
            REVIEW_FILE="notes/reviews/${TODAY_DATE}.md"
            TIMESTAMP=$(date +"%H:%M")
            
            # Extract content (everything after first line)
            PROGRESS_CONTENT=$(echo "$CONTENT" | tail -n +2 | sed '/^$/d')
            
            if [[ -f "$REVIEW_FILE" ]]; then
                # Append to review file
                echo "" >> "$REVIEW_FILE"
                echo "### Progress Update ($TIMESTAMP)" >> "$REVIEW_FILE"
                echo "$PROGRESS_CONTENT" >> "$REVIEW_FILE"
                print_info "  â€¢ Progress note â†’ Added to today's review"
                print_info "    $TIMESTAMP: $(echo "$PROGRESS_CONTENT" | head -1)"
            else
                print_error "  â€¢ No review file for today. Run 'bin/today' first"
            fi
            
            # Delete the progress note from inbox
            rm "$file"
            continue  # Skip the normal file move logic
        
        # Special case: Concerns file
        elif [[ "$TITLE" == "Concerns" ]] || [[ "$BASENAME" == *"concerns"* ]]; then
            # Remove title line and subsequent blank lines
            CLEANED_CONTENT=$(echo "$CONTENT" | awk '
                NR==1 && /^#.*[Cc]oncerns/ {next}  # Skip title line if it contains "concerns"
                /^$/ && !started {next}            # Skip leading blank lines
                {started=1; print}                 # Print everything else
            ')
            echo "$CLEANED_CONTENT" > "$file"  # Update content for move
            DEST_DIR="notes/concerns"
            DEST_FILE="$BASENAME"
            print_info "  â€¢ Concerns file â†’ concerns/$BASENAME"
        
        # Task files (contains checkboxes)
        elif echo "$CONTENT" | grep -qE "^-\s*\[[ x]\]"; then
            # Check if this file contains ONLY tasks (no other content)
            TASK_ONLY=true
            while IFS= read -r line; do
                # Skip empty lines
                if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*$ ]]; then
                    continue
                fi
                # Check if line is a task
                if ! echo "$line" | grep -qE "^-\s*\[[ x]\]"; then
                    TASK_ONLY=false
                    break
                fi
            done <<< "$CONTENT"
            
            if [[ "$TASK_ONLY" == "true" ]]; then
                # Append to consolidated tasks.md file
                TASKS_FILE="notes/tasks/tasks.md"
                mkdir -p notes/tasks 2>/dev/null
                
                # Simply append the tasks (no header, no timestamps)
                echo "$CONTENT" >> "$TASKS_FILE"
                
                print_info "  â€¢ Task-only file â†’ Appended to tasks/tasks.md"
                print_info "    Added $(echo "$CONTENT" | grep -c "^-\s*\[[ x]\]") task(s)"
                
                # Delete the original file from inbox
                rm "$file"
                continue  # Skip the normal file move logic
            else
                # Mixed content - keep as separate file
                DEST_DIR="notes/tasks"
                DEST_FILE="$BASENAME"
                print_info "  â€¢ Mixed task file â†’ tasks/$BASENAME"
            fi
        
        # Daily notes (default)
        else
            DEST_DIR="notes/daily"
            DEST_FILE="$BASENAME"
            print_info "  â€¢ Daily note â†’ daily/$BASENAME"
        fi
        
        # Create destination directory if needed
        mkdir -p "$DEST_DIR" 2>/dev/null
        
        # Move the file
        DEST_PATH="$DEST_DIR/$DEST_FILE"
        if [[ -f "$DEST_PATH" ]] && [[ "$DEST_FILE" != "streaks-today.md" ]]; then
            # File exists and it's not streaks (which we always overwrite)
            print_info "    File exists, skipping: $DEST_FILE"
            rm "$file"  # Remove from inbox since it's a duplicate
        else
            mv "$file" "$DEST_PATH" 2>/dev/null && {
                print_info "    âœ“ Filed to $DEST_PATH"
            } || {
                print_error "    Failed to move $BASENAME"
            }
        fi
    done <<< "$INBOX_FILES"
    
    # Commit any moved files
    if [[ -n $(git status --porcelain notes/ 2>/dev/null) ]]; then
        print_info "Committing filed notes..."
        git add notes/ 2>/dev/null
        git commit -m "Process inbox: file notes to appropriate directories" --quiet 2>/dev/null
        git push origin main --quiet 2>/dev/null
    fi
    
    return 0
}

# Archive completed tasks in tasks.md
archive_completed_tasks() {
    cd "$(dirname "$0")/.." || return 1
    
    TASKS_FILE="notes/tasks/tasks.md"
    
    # Check if tasks.md exists
    if [[ ! -f "$TASKS_FILE" ]]; then
        print_info "No tasks.md file found"
        return 0
    fi
    
    # Check if there are any completed tasks
    if ! grep -q "^\- \[x\]" "$TASKS_FILE" 2>/dev/null; then
        print_info "No completed tasks to archive"
        return 0
    fi
    
    # Count completed tasks
    COMPLETED_COUNT=$(grep -c "^\- \[x\]" "$TASKS_FILE")
    
    # Create temporary files
    ACTIVE_TASKS=$(mktemp)
    COMPLETED_TASKS=$(mktemp)
    
    # Separate active and completed tasks
    grep -v "^\- \[x\]" "$TASKS_FILE" > "$ACTIVE_TASKS" || true
    grep "^\- \[x\]" "$TASKS_FILE" > "$COMPLETED_TASKS" || true
    
    # Get today's date
    TODAY=$(date +%Y-%m-%d)
    
    # Rebuild the file
    {
        # Active tasks first (excluding Archive section and completed tasks)
        awk '/^# Archive/ {exit} {print}' "$TASKS_FILE" | grep -v "^\- \[x\]" | sed '/^$/d'
        
        # Add Archive section
        echo ""
        echo "# Archive"
        
        # Keep existing archive content (but remove today's section if it exists - we'll rebuild it)
        if grep -q "^# Archive" "$TASKS_FILE"; then
            awk -v today="$TODAY" '
                /^# Archive/ {in_archive=1; next}  # Skip the Archive header line
                in_archive && /^## / {
                    # Check if this is today'"'"'s section
                    if ($0 == "## " today) {
                        skip=1
                        next
                    } else {
                        skip=0
                        print
                        next
                    }
                }
                in_archive && skip {next}  # Skip lines in today'"'"'s section
                in_archive {print}  # Print other archive content
            ' "$TASKS_FILE" || true
        fi
        
        # Add today's completed tasks (only if there are new ones)
        if [[ -s "$COMPLETED_TASKS" ]]; then
            echo ""
            echo "## $TODAY"
            cat "$COMPLETED_TASKS"
        fi
    } > "${TASKS_FILE}.tmp"
    
    # Replace the original file
    mv "${TASKS_FILE}.tmp" "$TASKS_FILE"
    
    # Clean up temp files
    rm -f "$ACTIVE_TASKS" "$COMPLETED_TASKS"
    
    # Clean up old archived tasks (older than 30 days)
    THIRTY_DAYS_AGO=$(date -d "30 days ago" +%Y-%m-%d 2>/dev/null || date -v -30d +%Y-%m-%d 2>/dev/null || echo "")
    if [[ -n "$THIRTY_DAYS_AGO" ]]; then
        # This is complex to do in bash, so we'll keep it simple for now
        print_info "Archived $COMPLETED_COUNT completed task(s) to $TODAY section"
    else
        print_info "Archived $COMPLETED_COUNT completed task(s)"
    fi
    
    return 0
}

# Update cache statistics
update_cache_stats() {
    cd "$(dirname "$0")/.." || return 1
    
    # Check if cache database exists
    CACHE_DB=".notion-cache/notion-cache.db"
    if [[ ! -f "$CACHE_DB" ]]; then
        print_info "Cache database not found, initializing..."
        mkdir -p .notion-cache
        # Create an empty database with the required tables
        sqlite3 "$CACHE_DB" <<EOF
CREATE TABLE IF NOT EXISTS emails (
    id TEXT PRIMARY KEY,
    subject TEXT,
    from_address TEXT,
    date TEXT,
    has_been_replied_to INTEGER DEFAULT 0,
    text_content TEXT
);
CREATE TABLE IF NOT EXISTS task_cache (
    id TEXT PRIMARY KEY,
    title TEXT,
    due_date TEXT,
    stage TEXT,
    tags TEXT,
    description TEXT,
    last_edited_time TEXT
);
CREATE TABLE IF NOT EXISTS database_cache (
    id TEXT PRIMARY KEY,
    data TEXT
);
CREATE TABLE IF NOT EXISTS cache_metadata (
    key TEXT PRIMARY KEY,
    value TEXT
);
CREATE TABLE IF NOT EXISTS project_pillar_mapping (
    project_id TEXT,
    pillar_id TEXT
);
EOF
        print_info "Cache database initialized"
    fi
    
    # Get cache statistics
    cache_output=$(node -e "
        // Use environment variables already loaded by parent script
        import('./src/sqlite-cache.js').then(({default: DatabaseCache}) => {
            const cache = new DatabaseCache();
            const stats = cache.getStatistics();
            if (stats && stats.totalItems !== undefined) {
                console.log('Items: ' + stats.totalItems);
                console.log('Databases: ' + stats.totalDatabases);
                const sizeInMB = stats.cacheSize ? (stats.cacheSize / 1024 / 1024).toFixed(2) : '0.00';
                console.log('Size: ' + sizeInMB + ' MB');
            } else {
                console.log('Cache initialized');
            }
            cache.close();
        }).catch((err) => {
            console.error('Cache error: ' + err.message);
            process.exit(1);
        });
    " 2>&1)
    
    if [[ $? -eq 0 ]]; then
        if [[ -n "$cache_output" ]]; then
            while IFS= read -r line; do
                print_info "$line"
            done <<< "$cache_output"
        fi
        return 0
    else
        # Silently succeed - cache errors aren't critical
        return 0
    fi
}

# Parse command line arguments
case "${1:-}" in
    --notes)
        # Sync only GitHub notes and process inbox (replaces sync-notes script)
        print_header "ðŸ“ GitHub Notes"
        if sync_github_notes; then
            print_status "Notes synced successfully!"
        else
            print_error "Notes sync failed"
        fi
        
        # Also process inbox when syncing notes
        print_header "ðŸ“¥ Process Inbox"
        if process_inbox; then
            print_status "Inbox processed successfully!"
        else
            print_error "Inbox processing failed"
        fi
        
        # Archive completed tasks
        print_header "âœ… Archive Tasks"
        if archive_completed_tasks; then
            print_status "Tasks archived successfully!"
        fi
        ;;
    --quick)
        # Quick sync - only critical sources
        print_info "Quick sync mode - GitHub and Notion only"
        sync_github_notes
        sync_notion
        ;;
    --force)
        # Force full sync - bypasses smart caching
        print_info "Force sync mode - refreshing all data"
        FORCE_SYNC=true
        main
        ;;
    --quick-email)
        # Just sync last day of email quickly
        print_header "ðŸ“§ Quick Email Sync"
        EMAIL_BIN=""
        if [[ -f bin/email ]]; then
            EMAIL_BIN="bin/email"
        elif [[ -f bin/email-cli ]]; then
            EMAIL_BIN="bin/email-cli"
        fi
        if [[ -n "$EMAIL_BIN" ]]; then
            print_info "Downloading emails from last 24 hours..."
            timeout 30 $EMAIL_BIN download --days 1 | grep -E "Downloaded|Total|âœ…" || true
            print_status "Quick email sync complete"
        else
            print_error "Email CLI not found"
        fi
        ;;
    --help|-h)
        echo "Usage: sync [OPTIONS]"
        echo ""
        echo "Intelligently synchronize all data sources"
        echo ""
        echo "By default, uses smart caching:"
        echo "  â€¢ Only syncs Notion if cache >4 hours old"
        echo "  â€¢ Only syncs email if cache >4 hours old"
        echo "  â€¢ Uses efficient timeouts (20s for email)"
        echo ""
        echo "Options:"
        echo "  --notes       Sync GitHub notes only"
        echo "  --quick       Quick sync (GitHub and Notion only)"
        echo "  --force       Force full refresh (bypass cache)"
        echo "  --quick-email Download last 24 hours of email"
        echo "  --help        Show this help message"
        echo ""
        echo "Data sources synced:"
        echo "  â€¢ GitHub notes"
        echo "  â€¢ Notion database (cached for 4 hours)"
        echo "  â€¢ Todoist tasks (via Notion)"
        echo "  â€¢ Email inbox (cached for 4 hours)"
        echo "  â€¢ Calendar events"
        echo "  â€¢ Inbox processing (files from Drafts)"
        echo "  â€¢ Local cache"
        ;;
    *)
        main
        ;;
esac