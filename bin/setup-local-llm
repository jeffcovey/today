#!/bin/bash

echo "ü§ñ Local LLM Setup for Today CLI"
echo "================================"
echo ""
echo "This will help you set up a free, local AI to reduce Claude API usage."
echo ""

# Detect OS
OS="$(uname -s)"
ARCH="$(uname -m)"

# Check if Ollama is installed
if command -v ollama &> /dev/null; then
    echo "‚úÖ Ollama is already installed"
    
    # Test if ollama actually works
    if ! ollama --version &> /dev/null; then
        echo "‚ö†Ô∏è  Ollama binary found but cannot execute"
        echo ""
        echo "This usually means:"
        echo "  ‚Ä¢ Architecture mismatch (binary compiled for different CPU)"
        echo "  ‚Ä¢ Missing required libraries"
        echo ""
        echo "Current architecture: $(uname -m)"
        echo ""
        echo "To fix, reinstall Ollama for your architecture:"
        echo "  ‚Ä¢ For x86_64: curl -fsSL https://ollama.com/install.sh | sh"
        echo "  ‚Ä¢ For Docker: Consider using ollama/ollama Docker image"
        echo "  ‚Ä¢ Or build from source: https://github.com/ollama/ollama"
        exit 1
    fi
    
    echo ""
    echo "Available models:"
    ollama list
else
    echo "üì¶ Ollama is not installed."
    echo ""
    echo "Would you like to install Ollama? (y/n)"
    read -r INSTALL_OLLAMA
    
    if [[ "$INSTALL_OLLAMA" == "y" ]]; then
        # Check if we're in a container
        if [ -f /.dockerenv ] || [ -n "$CONTAINER" ]; then
            echo ""
            echo "‚ö†Ô∏è  Running in a container environment"
            echo ""
            echo "For containers, it's better to use Ollama as a separate service:"
            echo ""
            echo "Option 1: Use Docker Compose with ollama/ollama image"
            echo "Option 2: Run on host and expose port 11434 to container"
            echo ""
            echo "Attempting standard install anyway..."
        fi
        
        echo "Installing Ollama..."
        
        if [[ "$OS" == "Linux" ]] || [[ "$OS" == "Darwin" ]]; then
            # Official Ollama install script
            if curl -fsSL https://ollama.com/install.sh | sh; then
                echo "‚úÖ Ollama installed successfully"
            else
                echo "‚ùå Ollama installation failed"
                echo ""
                echo "For containers, consider running Ollama as a separate service:"
                echo "  docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama"
                exit 1
            fi
        else
            echo "Please visit https://ollama.com/download to install Ollama for your system"
            exit 1
        fi
    else
        echo "Skipping Ollama installation"
        exit 0
    fi
fi

echo ""
echo "üìö Recommended models for Today CLI (in order of preference):"
echo ""
echo "1. phi3 (3.8B) - Fast, good for simple tasks, low memory usage"
echo "2. tinyllama (1.1B) - Very fast, minimal memory, good enough for basic tasks"
echo "3. mistral (7B) - Better quality, more memory needed"
echo "4. llama3 (8B) - High quality, requires more resources"
echo ""

# Check which models are already installed
INSTALLED_MODELS=$(ollama list 2>/dev/null | tail -n +2 | awk '{print $1}')

if [[ -z "$INSTALLED_MODELS" ]]; then
    echo "No models currently installed."
else
    echo "Currently installed models:"
    echo "$INSTALLED_MODELS" | sed 's/^/  - /'
    echo ""
fi

echo "Would you like to download a model? (Enter number 1-4, or 'n' to skip)"
read -r MODEL_CHOICE

# Start Ollama server if not running
if ! pgrep -x "ollama" > /dev/null; then
    echo "Starting Ollama server..."
    if ! ollama serve > /dev/null 2>&1 &; then
        echo "‚ö†Ô∏è  Failed to start Ollama server"
        echo "You may need to start it manually with: ollama serve"
    else
        sleep 3  # Give it time to start
    fi
fi

MODEL_DOWNLOADED=false
case $MODEL_CHOICE in
    1)
        echo "Downloading phi3 model (about 2.3GB)..."
        if ollama pull phi3 2>/dev/null; then
            MODEL_DOWNLOADED=true
            echo "‚úÖ phi3 model downloaded successfully"
        else
            echo "‚ùå Failed to download phi3 model"
        fi
        ;;
    2)
        echo "Downloading tinyllama model (about 638MB)..."
        if ollama pull tinyllama 2>/dev/null; then
            MODEL_DOWNLOADED=true
            echo "‚úÖ tinyllama model downloaded successfully"
        else
            echo "‚ùå Failed to download tinyllama model"
        fi
        ;;
    3)
        echo "Downloading mistral model (about 4.1GB)..."
        if ollama pull mistral 2>/dev/null; then
            MODEL_DOWNLOADED=true
            echo "‚úÖ mistral model downloaded successfully"
        else
            echo "‚ùå Failed to download mistral model"
        fi
        ;;
    4)
        echo "Downloading llama3 model (about 4.7GB)..."
        if ollama pull llama3 2>/dev/null; then
            MODEL_DOWNLOADED=true
            echo "‚úÖ llama3 model downloaded successfully"
        else
            echo "‚ùå Failed to download llama3 model"
        fi
        ;;
    n|N)
        echo "Skipping model download"
        ;;
    *)
        echo "Invalid choice, skipping model download"
        ;;
esac

echo ""

# Check if we have any models installed
INSTALLED_COUNT=$(ollama list 2>/dev/null | tail -n +2 | wc -l)

if [ "$INSTALLED_COUNT" -gt 0 ] || [ "$MODEL_DOWNLOADED" = true ]; then
    echo "üéâ Setup complete!"
    echo ""
    echo "To test your local LLM:"
    MODEL_NAME=$(ollama list 2>/dev/null | tail -n +2 | head -1 | awk '{print $1}')
    if [ -n "$MODEL_NAME" ]; then
        echo "  ollama run $MODEL_NAME \"Hello, how are you?\""
    else
        echo "  ollama run <model_name> \"Hello, how are you?\""
    fi
    echo ""
    echo "The Today CLI will now automatically use your local LLM for:"
    echo "  - Daily summary recommendations (bin/update-summary)"
    echo "  - Simple email/task searches"
    echo "  - Basic intent classification"
    echo ""
    echo "This will help reduce your Claude API usage significantly!"
else
    echo "‚ö†Ô∏è  Setup incomplete"
    echo ""
    echo "Ollama is installed but no models are available."
    echo ""
    echo "Common issues:"
    echo "  1. The ollama binary may not be executable on this architecture"
    echo "  2. The ollama server may not be running"
    echo ""
    echo "Try manually:"
    echo "  1. Start server: ollama serve"
    echo "  2. In another terminal: ollama pull tinyllama"
    echo ""
    echo "Or install Ollama from source for your architecture:"
    echo "  https://github.com/ollama/ollama"
fi