#!/usr/bin/env python3
"""
Fetch ALL historical Toggl time entries using Reports API v3.
The API has a 366-day limit, so we fetch in yearly chunks.
"""

import sys
import json
import subprocess
from datetime import datetime, timedelta
import time

WORKSPACE_ID = "3208110"

def run_curl(start_date, end_date, first_row=1, page_size=1000):
    """Run curl command to fetch time entries."""
    cmd = [
        "npx", "dotenvx", "run", "--quiet", "--",
        "bash", "-c",
        f'''curl -s -u "$TOGGL_API_TOKEN:api_token" \\
            -H "Content-Type: application/json" \\
            -X POST \\
            "https://api.track.toggl.com/reports/api/v3/workspace/{WORKSPACE_ID}/search/time_entries" \\
            -d '{{"start_date": "{start_date}", "end_date": "{end_date}", "page_size": {page_size}, "first_row_number": {first_row}}}'
        '''
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    # Filter out dotenvx output lines (they contain '[dotenvx' or color codes)
    lines = result.stdout.split('\n')
    json_lines = [line for line in lines if not ('[dotenvx' in line or '\x1b[' in line)]
    stdout = '\n'.join(json_lines).strip()

    # Check if response is an error message (string instead of JSON array)
    if stdout.startswith('"'):
        # It's an error message
        error = stdout.strip().strip('"')
        return None, error

    try:
        data = json.loads(stdout)
        return data, None
    except json.JSONDecodeError:
        return None, f"Invalid JSON: {stdout[:100]}"

def fetch_year(year):
    """Fetch all entries for a given year."""
    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    print(f"\\nFetching {year}...")

    all_entries = []
    page = 1

    while True:
        first_row = (page - 1) * 1000 + 1
        data, error = run_curl(start_date, end_date, first_row)

        if error:
            print(f"  Error: {error}")
            break

        if not data:
            break

        entry_count = len(data)
        print(f"  Page {page}: {entry_count} entries")

        all_entries.extend(data)

        # If we got less than page size, we're done with this year
        if entry_count < 1000:
            break

        page += 1
        time.sleep(0.5)  # Be nice to the API

    print(f"  Total for {year}: {len(all_entries)} entries")
    return all_entries

def main():
    # Fetch from 2015 to 2025 (adjust as needed)
    start_year = 2015
    end_year = 2025

    all_time_entries = []

    print(f"Fetching Toggl data from {start_year} to {end_year}...")

    for year in range(start_year, end_year + 1):
        year_entries = fetch_year(year)
        all_time_entries.extend(year_entries)

    print(f"\\nTotal entries fetched: {len(all_time_entries)}")

    # Save to file
    output_file = "/tmp/toggl_all_historical.json"
    with open(output_file, 'w') as f:
        json.dump(all_time_entries, f, indent=2)

    print(f"Saved to: {output_file}")

    # Show date range
    if all_time_entries:
        dates = [entry.get('start') for entry in all_time_entries if entry.get('start')]
        if dates:
            print(f"Date range: {min(dates)} to {max(dates)}")

if __name__ == '__main__':
    main()
